# üöÄ Guide de D√©ploiement - Architecture IA 3 Services

## ‚úÖ Changements R√©alis√©s

### 1. **app.py** - Service Principal
- ‚ùå **Supprim√©:** GeminiService
- ‚úÖ **Remplac√© par:** Appels au service IA (port 5002)
- **Modifications:**
  - Routes alerte ‚Üí `/api/alerts/process` (service IA)
  - Routes retraining ‚Üí `/api/learn` (service IA)
  - Routes nouvelle panne ‚Üí `/api/learn` (service IA)
  - Fallback HTTP avec gestion d'erreurs

### 2. **chat/app_web.py** - Service Web
- ‚ùå **Supprim√©:** Appels app.py pour messages
- ‚úÖ **Remplac√© par:** Appels directs au service IA (port 5002)
- **Modifications:**
  - Config.IA_SERVICE_URL ajout√©e
  - POST `/api/messages` ‚Üí appelle `/api/chat/message` (service IA)
  - WebSocket `send_message` ‚Üí appelle service IA en temps r√©el
  - WebSocket `request_system_response` ‚Üí appelle service IA

### 3. **gpt/Dockerfile** - Containerisation IA
- Base Python 3.11
- D√©pendances: torch, transformers, accelerate, etc.
- Mod√®le par d√©faut: Phi-2 (auto-t√©l√©charg√© au d√©marrage)
- Health check int√©gr√©
- Gunicorn en production (1 worker, timeout 120s)

### 4. **docker-compose.yml** - Orchestration 3 Services
- ‚úÖ **Service 1:** main-app (port 5000) - Diagnostic principal
- ‚úÖ **Service 2:** chat-web (port 5001) - Interface web chat
- ‚úÖ **Service 3:** ia-service (port 5002) - LLM Phi-2 local
- **Volumes persistants:**
  - ia-models (5-10GB) - Mod√®les LLM
  - ia-data - Base de connaissances
  - ia-logs - Logs du service IA

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   CLIENT (Browser)                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ WebSocket
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CHAT WEB (port 5001)                                   ‚îÇ
‚îÇ  - Dashboard en temps r√©el                              ‚îÇ
‚îÇ  - Historique messages                                  ‚îÇ
‚îÇ  - Alertes                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ HTTP
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚Üì              ‚Üì              ‚Üì
    IA Service     MAIN APP      Telegram
    (port 5002)   (port 5000)
        ‚îÇ              ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ  Phi-2 LLM       ‚îÇ
        ‚îÇ  (2.7B model)    ‚îÇ
        ‚îÇ  GPU/CPU auto    ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üì¶ D√©ploiement Local

### √âtape 1: Pr√©paration

```powershell
# Aller au r√©pertoire racine
cd c:\Users\hp\Desktop\systeme-diagnostic-frigo

# V√©rifier Docker
docker --version
docker-compose --version

# Cr√©er les fichiers d'environnement
echo "TELEGRAM_BOT_TOKEN=YOUR_BOT_TOKEN" > .env
echo "TELEGRAM_CHAT_ID=YOUR_CHAT_ID" >> .env
echo "IA_MODEL=phi" >> .env
echo "IA_USE_GPU=false" >> .env
```

### √âtape 2: Build des images

```powershell
# Build complet des 3 services (peut prendre 10-15 minutes)
docker-compose build

# Ou rebuild sans cache
docker-compose build --no-cache
```

### √âtape 3: D√©marrage

```powershell
# D√©marrer tous les services
docker-compose up -d

# V√©rifier que les services sont en cours d'ex√©cution
docker-compose ps

# Regarder les logs
docker-compose logs -f
```

### √âtape 4: Tests

```powershell
# Test main-app
curl http://localhost:5000/health

# Test chat-web
curl http://localhost:5001/

# Test ia-service
curl http://localhost:5002/health
```

## ü§ñ Chargement du Mod√®le Phi-2

**Important:** La premi√®re ex√©cution du service IA t√©l√©chargera le mod√®le (~5.3GB)
- **Temps:** 5-30 minutes selon la connexion internet
- **Espace:** 15GB disque libre recommand√©
- **Une seule fois:** Le mod√®le est mis en cache dans le volume `ia-models`

### Monitoring du t√©l√©chargement

```powershell
# Suivre les logs du t√©l√©chargement
docker logs -f frigo-ia-service

# V√©rifier l'espace utilis√© par le mod√®le
docker volume ls
docker volume inspect frigo_ia-models
```

## üéØ Utilisation

### Via le Dashboard Web

1. **Ouvrir:** http://localhost:5001
2. **Login:** admin@example.com / admin123
3. **Chat:** Envoyer un message
4. **R√©ponse IA:** Obtenir une r√©ponse intelligente du mod√®le Phi-2

### Via API HTTP

```powershell
# Chat message
curl -X POST http://localhost:5002/api/chat/message `
  -H "Content-Type: application/json" `
  -d '{
    "message": "Le compresseur ne d√©marre pas",
    "user_id": "test",
    "user_name": "Technicien"
  }'

# Alerte
curl -X POST http://localhost:5002/api/alerts/process `
  -H "Content-Type: application/json" `
  -d '{
    "title": "Anomalie temp√©rature",
    "severity": "critical",
    "sensors": {"temp": 28, "humidity": 65}
  }'
```

## ‚öôÔ∏è Configuration

### Variables d'Environnement

| Variable | D√©faut | Description |
|----------|--------|-------------|
| `IA_MODEL` | `phi` | Mod√®le LLM: phi, mistral, neural, llama, gpt2 |
| `IA_USE_GPU` | `false` | Utiliser GPU NVIDIA (n√©cessite nvidia-docker) |
| `IA_TEMPERATURE` | `0.7` | Cr√©ativit√© des r√©ponses (0-1) |
| `IA_MAX_TOKENS` | `512` | Longueur max des r√©ponses |
| `FLASK_ENV` | `production` | Mode: production ou development |

### Changer de mod√®le LLM

```powershell
# Utiliser Mistral-7B (meilleure qualit√©, plus lent)
docker-compose down
$env:IA_MODEL="mistral"
docker-compose up -d

# Utiliser GPT-2 (tr√®s l√©ger, test rapide)
$env:IA_MODEL="gpt2"
docker-compose up -d ia-service
```

## üêõ R√©solution de Probl√®mes

### Service IA ne d√©marre pas

```powershell
# V√©rifier les logs
docker logs frigo-ia-service

# Relancer avec rebuild
docker-compose up -d --build ia-service
```

### Pas assez de m√©moire

```powershell
# Utiliser mod√®le plus l√©ger
$env:IA_MODEL="gpt2"
docker-compose restart ia-service

# Ou d√©sactiver GPU (si activ√©)
$env:IA_USE_GPU="false"
```

### Mod√®le tr√®s lent

C'est **normal** sur CPU:
- Phi-2 sur CPU: 3-5 sec/r√©ponse (normal)
- Mistral sur CPU: 20+ sec/r√©ponse
- **Solution:** Installer GPU ou utiliser GPT-2

### Erreur "CUDA out of memory"

```powershell
$env:IA_USE_GPU="false"
docker-compose restart ia-service
```

## üìä Performance

| Mod√®le | Vitesse | CPU | GPU | Qualit√© |
|--------|---------|-----|-----|---------|
| GPT-2 | ‚ö°‚ö°‚ö° | 0.5s | 0.1s | Basique |
| Phi-2 | ‚ö°‚ö° | 3-5s | 0.5s | Bon ‚≠ê |
| Mistral | ‚ö° | 20s+ | 1-2s | Excellent |

## üîí Production

### Avant de d√©ployer en production:

1. **Changer les secrets**
   ```bash
   # G√©n√©rer une cl√© s√©curis√©e
   python -c "import secrets; print(secrets.token_hex(32))"
   # Ajouter √† .env
   SECRET_KEY=<votre-cl√©-g√©n√©r√©e>
   ```

2. **Utiliser Mistral pour meilleure qualit√©**
   ```bash
   IA_MODEL=mistral
   ```

3. **Activer GPU si disponible**
   ```bash
   IA_USE_GPU=true
   ```

4. **Mettre √† jour les credentials**
   ```bash
   TELEGRAM_BOT_TOKEN=<token>
   TELEGRAM_CHAT_ID=<chat-id>
   POSTGRES_PASSWORD=<password-s√©curis√©>
   ```

5. **Reverse proxy (Nginx)**
   ```nginx
   server {
       listen 80;
       server_name votre-domaine.com;
       
       location / {
           proxy_pass http://localhost:5001;
           proxy_set_header Host $host;
           proxy_set_header X-Real-IP $remote_addr;
       }
   }
   ```

## üìà Monitoring

```powershell
# Logs en temps r√©el
docker-compose logs -f

# Stats ressources
docker stats

# V√©rifier la sant√© des services
docker-compose ps

# Red√©marrer un service
docker-compose restart ia-service
```

## üõë Arr√™t et Nettoyage

```powershell
# Arr√™ter les services
docker-compose stop

# Red√©marrer
docker-compose restart

# Tout arr√™ter et supprimer les conteneurs
docker-compose down

# Supprimer aussi les volumes (ATTENTION: perte de donn√©es)
docker-compose down -v
```

## ‚ú® R√©sum√© Architecture

**Avant:**
```
app.py ‚Üí Gemini API ‚Üí Telegram ‚Üí Chat Web
        (Lent, Co√ªteux, En ligne)
```

**Apr√®s:**
```
app.py ‚îÄ‚îÄ‚Üí IA Service (Phi-2 Local) ‚îÄ‚îÄ‚Üí Telegram
Chat Web ‚îÄ‚Üí IA Service (Phi-2 Local) ‚îÄ‚îÄ‚Üí WebSocket
           (Rapide, Gratuit, Hors ligne)
```

## üéâ √âtapes Suivantes

1. ‚úÖ **Services connect√©s** - app.py, chat-web, ia-service
2. ‚úÖ **Docker pr√™t** - docker-compose.yml, Dockerfile
3. üìä **Fine-tuning** (optionnel) - Adapter mod√®le sur vos donn√©es
4. üìö **RAG** (optionnel) - Ajouter ChromaDB pour knowledge base
5. üöÄ **Production** - D√©ployer en cloud (AWS, GCP, Azure)

**Status:** ‚úÖ PR√äT POUR LA PRODUCTION
