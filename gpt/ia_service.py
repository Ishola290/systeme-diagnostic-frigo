"""
Service IA Local - Point central de traitement
Architecture modulaire: Support multi-mod√®les (phi-2, mistral, ollama, gpt2)
S√©lection automatique selon GPU/CPU disponible
Pr√™t pour r√©entra√Ænement avec donn√©es domaine frigo
"""

import os
import json
import logging
from datetime import datetime
from pathlib import Path
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import requests

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class IAConfig:
    """Configuration du service IA - Support multi-mod√®les"""
    
    # ========== MOD√àLES DISPONIBLES ==========
    # Format: 'nom_court': {'hf_id': 'huggingface_id', 'local_folder': 'dossier_local', 'production': bool}
    MODEL_OPTIONS = {
        'phi2': {
            'hf_id': 'microsoft/phi-2',
            'local_folder': 'phi-2',
            'production': True,
            'description': 'Phi-2: Petit, rapide, bon (2.7B params)'
        },
        'mistral': {
            'hf_id': 'mistralai/Mistral-7B-Instruct-v0.1',
            'local_folder': 'mistral-7b',
            'production': True,
            'description': 'Mistral-7B: √âquilibr√© (7B params)'
        },
        'neural': {
            'hf_id': 'Intel/neural-chat-7b-v3-1',
            'local_folder': 'neural-chat-7b',
            'production': True,
            'description': 'Neural-Chat: Optimis√© pour chat'
        },
        'gpt2': {
            'hf_id': 'openai/gpt2',
            'local_folder': 'gpt2',
            'production': False,  # Fallback/dev seulement
            'description': 'GPT-2: Ultra-l√©ger (125M params)'
        },
        'ollama': {
            'hf_id': None,  # Utilise ollama API
            'local_folder': None,
            'production': True,
            'description': 'Ollama: Local inference engine'
        }
    }
    
    # S√©lection automatique selon ressources
    MODEL_SELECTION_STRATEGY = {
        'has_gpu_high_memory': 'mistral',    # GPU avec beaucoup VRAM
        'has_gpu_low_memory': 'phi2',        # GPU limit√©
        'cpu_only': 'phi2',                  # CPU seulement (phi2 optimis√© CPU)
        'ollama_available': 'ollama',        # Si ollama est disponible
        'fallback': 'gpt2'                   # Dernier recours
    }
    
    # Param√®tres LLM - Optimis√©s pour production
    MAX_TOKENS = 120
    TEMPERATURE = 0.6
    TOP_P = 0.90
    
    # Contexte persistant
    CONTEXT_SIZE = 5
    
    # Bases de donn√©es
    DB_PATH = Path(__file__).parent / "data"
    CACHE_PATH = Path(__file__).parent / "cache"
    MODELS_PATH = Path(__file__).parent.parent / "models"
    
    def __init__(self):
        self.DB_PATH.mkdir(exist_ok=True)
        self.CACHE_PATH.mkdir(exist_ok=True)
        self.MODELS_PATH.mkdir(exist_ok=True)

class IAService:
    """Service IA principal - Architecture modulaire multi-mod√®les"""
    
    def __init__(self, model_name=None):
        self.config = IAConfig()
        self.model = None
        self.tokenizer = None
        self.text_generator = None
        self.conversation_history = []
        self.knowledge_base = {}
        self.model_info = {}
        
        # S√©lection intelligente si pas sp√©cifi√©
        if model_name is None:
            model_name = self._auto_select_model()
        
        self.model_name = model_name
        logger.info(f"ü§ñ Initialisation service IA")
        logger.info(f"üìã Mod√®le demand√©: {model_name}")
        
        self._load_model()
        self._load_knowledge_base()
    
    def _auto_select_model(self):
        """S√©lection automatique selon ressources disponibles"""
        logger.info("üîç D√©tection des ressources disponibles...")
        
        # V√©rifier ollama
        try:
            requests.get('http://localhost:11434/api/tags', timeout=2)
            logger.info("‚úÖ Ollama d√©tect√© - utilisation recommand√©e")
            return 'ollama'
        except:
            pass
        
        # V√©rifier GPU
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            logger.info(f"‚úÖ GPU disponible: {gpu_memory:.1f}GB")
            if gpu_memory > 10:
                logger.info("üìä GPU haute m√©moire -> Mistral-7B")
                return 'mistral'
            else:
                logger.info("üìä GPU m√©moire limit√©e -> Phi-2")
                return 'phi2'
        
        # CPU seulement
        logger.info("üìä CPU seulement -> Phi-2 (optimis√© CPU)")
        return 'phi2'
    
    def _load_model(self):
        """Charger le mod√®le LLM - Support multi-mod√®les avec fallback"""
        try:
            # Supporter anciens noms (mapping backward compatibility)
            model_name = self._normalize_model_name(self.model_name)
            
            if model_name not in self.config.MODEL_OPTIONS:
                logger.error(f"‚ùå Mod√®le inconnu: {model_name}")
                logger.info(f"Mod√®les disponibles: {list(self.config.MODEL_OPTIONS.keys())}")
                return False
            
            model_cfg = self.config.MODEL_OPTIONS[model_name]
            logger.info(f"üìã Configuration: {model_cfg['description']}")
            
            # Cas sp√©cial: Ollama
            if model_name == 'ollama':
                return self._load_ollama_model()
            
            # Cas standard: HuggingFace
            return self._load_huggingface_model(model_name, model_cfg)
            
        except Exception as e:
            logger.error(f"‚ùå Erreur chargement mod√®le: {e}")
            logger.warning("‚ö†Ô∏è Utilisation du mode fallback (r√©ponses intelligentes)")
            self.model = None
            return False
    
    def _normalize_model_name(self, name):
        """Normaliser les noms de mod√®les (backward compat)"""
        mapping = {'phi': 'phi2', 'gpt': 'gpt2'}
        return mapping.get(name, name)
    
    def _load_huggingface_model(self, model_name, model_cfg):
        """Charger un mod√®le HuggingFace"""
        hf_id = model_cfg['hf_id']
        local_folder = model_cfg['local_folder']
        
        # 1. V√©rifier mod√®le local d'abord
        local_path = self.config.MODELS_PATH / local_folder if local_folder else None
        use_local = local_path and local_path.exists()
        
        if use_local:
            logger.info(f"üìÅ Mod√®le local trouv√©: {local_path}")
            model_id = str(local_path)
        else:
            # 2. V√©rifier env var
            env_path = os.environ.get('HF_LOCAL_MODEL_PATH')
            if env_path:
                env_model_path = Path(env_path)
                if env_model_path.exists():
                    logger.info(f"üìÅ Mod√®le depuis HF_LOCAL_MODEL_PATH: {env_model_path}")
                    model_id = str(env_model_path)
                    use_local = True
                else:
                    logger.warning(f"‚ö†Ô∏è HF_LOCAL_MODEL_PATH introuvable: {env_path}")
            else:
                model_id = hf_id
        
        # 3. V√©rifier connectivit√© si t√©l√©chargement
        if not use_local:
            try:
                requests.get('https://huggingface.co', timeout=3)
                logger.info("‚úÖ Connectivit√© HuggingFace OK")
            except:
                logger.error("‚ùå Pas d'acc√®s √† huggingface.co")
                if model_name == 'phi2' or model_name == 'gpt2':
                    logger.info("üí° Fallback sur r√©ponses intelligentes")
                    return False
        
        # 4. Charger le mod√®le
        try:
            device = "cuda" if torch.cuda.is_available() else "cpu"
            logger.info(f"üñ•Ô∏è Device: {device.upper()}")
            
            logger.info(f"‚è≥ Chargement tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_id,
                trust_remote_code=True,
                local_files_only=use_local
            )
            
            logger.info(f"‚è≥ Chargement mod√®le {model_name}...")
            load_kwargs = {
                'trust_remote_code': True,
                'device_map': 'auto' if torch.cuda.is_available() else None,
            }
            
            if use_local:
                load_kwargs['local_files_only'] = True
            
            if torch.cuda.is_available():
                load_kwargs['torch_dtype'] = torch.float16
            
            self.model = AutoModelForCausalLM.from_pretrained(model_id, **load_kwargs)
            self.model.eval()
            
            # Cr√©er le pipeline
            self.text_generator = pipeline(
                'text-generation',
                model=self.model,
                tokenizer=self.tokenizer,
                device=0 if torch.cuda.is_available() else -1
            )
            
            logger.info(f"‚úÖ Mod√®le {model_name} charg√© avec succ√®s")
            self.model_info = {'name': model_name, 'device': device}
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erreur chargement HuggingFace: {e}")
            return False
    
    def _load_ollama_model(self):
        """Charger via Ollama API"""
        try:
            response = requests.get('http://localhost:11434/api/tags', timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                if models:
                    logger.info(f"‚úÖ Ollama disponible avec {len(models)} mod√®le(s)")
                    logger.info(f"üìã Mod√®les: {[m['name'] for m in models]}")
                    self.text_generator = None  # Sera utilis√© diff√©remment
                    self.model_info = {'name': 'ollama', 'api': 'localhost:11434'}
                    return True
                else:
                    logger.error("‚ùå Aucun mod√®le dans Ollama")
                    return False
            else:
                logger.error(f"‚ùå Ollama API erreur: {response.status_code}")
                return False
        except Exception as e:
            logger.error(f"‚ùå Impossible atteindre Ollama: {e}")
            return False

            # Par d√©faut, charger depuis models/{model_name} (chemin relatif au dossier gpt/)
            # Sinon, chercher dans HF_LOCAL_MODEL_PATH / LOCAL_MODEL_DIR
            # Priorit√©: env var > dossier local models/ > t√©l√©chargement HF
            use_local = False
            
            # 1. V√©rifier si variable d'environnement est d√©finie
            local_model_path = os.environ.get('HF_LOCAL_MODEL_PATH') or os.environ.get('LOCAL_MODEL_DIR')
            if local_model_path:
                local_path = Path(local_model_path)
                if local_path.exists():
                    logger.info(f"üìÅ Chargement mod√®le depuis env HF_LOCAL_MODEL_PATH: {local_path}")
                    model_id = str(local_path)
                    use_local = True
                else:
                    logger.warning(f"‚ö†Ô∏è Chemin env HF_LOCAL_MODEL_PATH non trouv√©: {local_model_path}")
            
            # 2. Si pas d'env var ou chemin invalide, essayer models/{model_name} (chemin relatif)
            if not use_local:
                # Mapping entre noms mod√®les courts et dossiers
                model_folder_mapping = {
                    'phi': 'phi-2',
                    'mistral': 'mistral-7b',
                    'llama': 'llama-2-7b',
                    'neural': 'neural-chat-7b'
                }
                folder_name = model_folder_mapping.get(self.model_name, self.model_name)
                # Chemin relatif: depuis gpt/ vers ../models/{folder_name}
                default_local = Path(__file__).parent.parent / "models" / folder_name
                if default_local.exists():
                    logger.info(f"üìÅ Chargement mod√®le depuis chemin par d√©faut: {default_local}")
                    model_id = str(default_local)
                    use_local = True
                else:
                    logger.warning(f"‚ö†Ô∏è Dossier mod√®le par d√©faut non trouv√©: {default_local} - tentative t√©l√©chargement HF")

            # V√©rifier rapidement la connectivit√© vers huggingface pour √©viter longues tentatives
            hf_online = True
            if not use_local:
                try:
                    requests.get('https://huggingface.co', timeout=3)
                except Exception:
                    hf_online = False
                    logger.warning("‚ö†Ô∏è Impossible de contacter huggingface.co (v√©rifiez la connexion r√©seau / DNS / proxy).\n"+
                                   "Si vous souhaitez charger un mod√®le depuis un dossier local, d√©finissez HF_LOCAL_MODEL_PATH.")
            
            # D√©terminer le device (GPU ou CPU)
            device = "cuda" if torch.cuda.is_available() else "cpu"
            logger.info(f"üñ• Device: {device.upper()}")
            
            # Si pas en ligne et pas de mod√®le local, on stoppe pour √©viter de longues retries
            if not use_local and not hf_online:
                raise RuntimeError("Pas d'acc√®s r√©seau √† huggingface.co et aucun mod√®le local fourni")

            # Charger le tokenizer
            logger.info(f"‚è≥ Chargement du tokenizer pour {model_id}...")
            tokenizer_kwargs = {
                'trust_remote_code': True,
            }
            if use_local:
                tokenizer_kwargs['local_files_only'] = True
            else:
                # param dtype seulement si GPU
                tokenizer_kwargs['torch_dtype'] = torch.float16 if torch.cuda.is_available() else torch.float32

            self.tokenizer = AutoTokenizer.from_pretrained(
                model_id,
                **tokenizer_kwargs
            )
            
            # Charger le mod√®le
            logger.info(f"‚è≥ Chargement du mod√®le {model_id}...")
            
            # Options de chargement selon les ressources disponibles
            load_kwargs = {
                'trust_remote_code': True,
                'device_map': 'auto' if torch.cuda.is_available() else None,
            }
            
            # Utiliser quantization pour √©conomiser la m√©moire si GPU disponible
            if torch.cuda.is_available():
                load_kwargs['torch_dtype'] = torch.float16
                try:
                    from transformers import BitsAndBytesConfig
                    load_kwargs['quantization_config'] = BitsAndBytesConfig(
                        load_in_4bit=True,
                        bnb_4bit_compute_dtype=torch.float16
                    )
                except:
                    pass  # Fallback sans quantization
            
            if use_local:
                load_kwargs['local_files_only'] = True

            self.model = AutoModelForCausalLM.from_pretrained(
                model_id,
                **load_kwargs
            )
            
            # Mettre le mod√®le en mode eval
            self.model.eval()
            
            # Cr√©er le pipeline de g√©n√©ration
            self.text_generator = pipeline(
                'text-generation',
                model=self.model,
                tokenizer=self.tokenizer,
                device=0 if torch.cuda.is_available() else -1
            )
            
            logger.info(f"‚úÖ Mod√®le {self.model_name} charg√© avec succ√®s sur {device}")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur chargement mod√®le: {e}")
            # Conseils actionnables
            logger.warning("‚ö† Utilisation du mode simulation (sans vrai mod√®le)")
            logger.info("Conseils: 1) V√©rifiez votre connexion Internet / DNS; 2) Si vous √™tes derri√®re un proxy, exportez HTTP_PROXY/HTTPS_PROXY;\n"+
                        "3) Pour de meilleures performances de t√©l√©chargement, installez 'hf_xet': `pip install huggingface_hub[hf_xet]`;\n"+
                        "4) Pour √©viter les t√©l√©chargements, pr√©-t√©l√©chargez le mod√®le et d√©finissez HF_LOCAL_MODEL_PATH vers le dossier du mod√®le.")

            # Tentative rapide: si pas d'acc√®s HF et aucun mod√®le local fourni,
            # essayer de charger un mod√®le local l√©ger 'gpt2' s'il est pr√©sent dans le cache
            try:
                fallback_id = 'gpt2'
                logger.info(f"üîÅ Tentative de chargement fallback local: {fallback_id}")
                # Charger tokenizer et mod√®le uniquement √† partir des fichiers locaux
                tk = AutoTokenizer.from_pretrained(fallback_id, local_files_only=True)
                mdl = AutoModelForCausalLM.from_pretrained(fallback_id, local_files_only=True)
                mdl.eval()
                self.tokenizer = tk
                self.model = mdl
                self.text_generator = pipeline('text-generation', model=self.model, tokenizer=self.tokenizer, device=-1)
                logger.info(f"‚úÖ Mod√®le de fallback '{fallback_id}' charg√© depuis le cache local")
                return
            except Exception:
                logger.info("‚ÑπÔ∏è Aucun mod√®le de fallback local disponible, maintien du mode simulation")

            self.model = None
            self.text_generator = None
    
    def _load_knowledge_base(self):
        """Charger la base de connaissances"""
        try:
            kb_file = self.config.DB_PATH / "knowledge_base.json"
            if kb_file.exists():
                with open(kb_file, 'r', encoding='utf-8') as f:
                    self.knowledge_base = json.load(f)
                logger.info(f"‚úÖ Base de connaissances charg√©e: {len(self.knowledge_base)} entr√©es")
            else:
                logger.info("‚Ñπ Pas de base de connaissances existante")
        except Exception as e:
            logger.error(f"‚ùå Erreur chargement KB: {e}")
    
    def process_chat_message(self, message, user_id=None):
        """
        Traiter un message du chat
        
        Args:
            message (str): Message de l'utilisateur
            user_id (str): ID de l'utilisateur
            
        Returns:
            dict: R√©ponse du service IA
        """
        try:
            logger.info(f"üí¨ Message re√ßu: {message[:50]}...")
            
            # 1. Analyser le message
            intent = self._analyze_intent(message)
            
            # 2. R√©cup√©rer le contexte
            context = self._get_context(message, user_id)
            
            # 3. G√©n√©rer la r√©ponse
            response = self._generate_response(message, context, intent)
            
            # 4. Sauvegarder dans l'historique
            self._save_to_history(user_id, message, response)
            
            return {
                'success': True,
                'response': response,
                'intent': intent,
                'timestamp': datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"‚ùå Erreur traitement message: {e}")
            return {
                'success': False,
                'error': str(e),
                'response': 'Erreur lors du traitement du message'
            }
    
    def process_alert(self, alert_data):
        """
        Traiter une alerte de app.py
        
        Args:
            alert_data (dict): Donn√©es d'alerte
            
        Returns:
            dict: Alerte trait√©e et enrichie
        """
        try:
            logger.info(f"üö® Alerte re√ßue: {alert_data.get('title', 'N/A')}")
            
            # 1. Analyser l'alerte
            severity = self._analyze_alert_severity(alert_data)
            
            # 2. Chercher des solutions
            solutions = self._find_solutions(alert_data)
            
            # 3. Enrichir l'alerte
            enriched_alert = {
                **alert_data,
                'processed': True,
                'severity_score': severity,
                'suggested_solutions': solutions,
                'processed_at': datetime.now().isoformat()
            }
            
            return enriched_alert
        except Exception as e:
            logger.error(f"‚ùå Erreur traitement alerte: {e}")
            return alert_data  # Retourner l'alerte originale en cas d'erreur
    
    def _analyze_intent(self, message):
        """Analyser l'intention du message"""
        message_lower = message.lower()
        
        intents = {
            'diagnostic': ['diagnostic', 'diagnose', 'problem', 'issue'],
            'solution': ['fix', 'repair', 'solution', 'how to'],
            'alert': ['alert', 'error', 'warning', 'critical'],
            'info': ['what', 'why', 'how', 'explain'],
            'learn': ['learn', 'train', 'remember'],
        }
        
        for intent, keywords in intents.items():
            if any(kw in message_lower for kw in keywords):
                return intent
        
        return 'general'
    
    def _get_context(self, message, user_id):
        """R√©cup√©rer le contexte pertinent"""
        context = {
            'message': message,
            'user_id': user_id,
            'history': self.conversation_history[-self.config.CONTEXT_SIZE:],
            'knowledge': self._search_knowledge_base(message)
        }
        return context
    
    def _search_knowledge_base(self, query):
        """Chercher dans la base de connaissances"""
        # Sera impl√©ment√© avec recherche s√©mantique
        results = []
        for key, value in self.knowledge_base.items():
            if any(word in query.lower() for word in key.split()):
                results.append(value)
        return results[:3]  # Top 3 r√©sultats
    
    def _generate_response(self, message, context, intent):
        """G√©n√©rer une r√©ponse avec le mod√®le LLM - MODE ULTRA-RAPIDE CPU"""
        try:
            if self.text_generator is None:
                logger.warning("‚ö† Mod√®le non disponible, r√©ponse de fallback")
                return self._generate_fallback_response(message, intent)
            
            # Sur CPU, mode ultra-rapide: r√©ponses tr√®s courtes (40 tokens max)
            logger.info("üß† G√©n√©ration rapide CPU...")
            
            # Prompt structur√© pour √©viter le bruit
            prompt = self._build_prompt(message, context, intent)
            
            # Param√®tres ultra-optimis√©s pour CPU + qualit√© + rapidit√©
            generation_params = {
                'max_new_tokens': 40,  # Ultra-court: 40 tokens
                'temperature': 0.3,    # Plus d√©terministe (moins al√©atoire)
                'top_p': 0.85,
                'do_sample': False,    # Greedy decoding (plus rapide, plus coh√©rent)
                'pad_token_id': self.tokenizer.eos_token_id if self.tokenizer else 50256,
            }
            
            try:
                outputs = self.text_generator(
                    prompt,
                    **generation_params
                )
            except Exception as e:
                logger.warning(f"‚ö† G√©n√©ration √©chou√©e ({e}), fallback")
                return self._generate_fallback_response(message, intent)
            
            if not outputs or len(outputs) == 0:
                return self._generate_fallback_response(message, intent)
                
            full_text = outputs[0].get('generated_text', '')
            response = full_text[len(prompt):].strip()
            
            # Nettoyer et limiter
            response = response.replace('\n', ' ')[:150]
            response = response.strip()
            
            if not response or len(response) < 3:
                return self._generate_fallback_response(message, intent)
            
            logger.info(f"‚úÖ R√©ponse: {response[:70]}...")
            return response
            
        except Exception as e:
            logger.error(f"‚ùå Erreur g√©n√©ration: {e}")
            return self._generate_fallback_response(message, intent)
    
    def _generate_fallback_response(self, message, intent):
        """G√©n√©rer une r√©ponse de fallback intelligente - PRIORIT√â sur gpt2 al√©atoire"""
        message_lower = message.lower()
        
        # R√©ponses intelligentes bas√©es sur les mots-cl√©s
        if any(word in message_lower for word in ['panne', 'probl√®me', 'erreur', 'error', 'problem', 'fail']):
            return "üî¥ Diagnostic d'alerte d√©tect√©.\n\nEtapes recommand√©es:\n1. V√©rifier l'alimentation\n2. Contr√¥ler la temp√©rature\n3. Inspecter les connexions\n4. Consulter les logs d√©taill√©s"
        
        elif any(word in message_lower for word in ['temp√©rature', 'temperature', 'temp', 'chaud', 'froid', 'heat', 'cold']):
            return "üå°Ô∏è Probl√®me de temp√©rature identifi√©.\n\nActions:\n1. V√©rifier le thermostat\n2. Nettoyer les filtres √† air\n3. V√©rifier la circulation d'air\n4. Contr√¥ler le compresseur"
        
        elif any(word in message_lower for word in ['√©lectrique', 'electrical', 'puissance', 'power', 'courant', 'current']):
            return "‚ö° Probl√®me d'alimentation d√©tect√©.\n\nV√©rifications:\n1. Contr√¥ler le fusible/disjoncteur\n2. Mesurer la tension (230V)\n3. V√©rifier le c√¢blage\n4. Tester l'interrupteur"
        
        elif any(word in message_lower for word in ['bruit', 'noise', 'son', 'sound', 'vibration']):
            return "üîä Anomalie sonore d√©tect√©e.\n\nCauses possibles:\n1. Compresseur us√©\n2. Ventilateur d√©faillant\n3. Vibrations m√©caniques\n4. Accumulation de givre"
        
        elif any(word in message_lower for word in ['fuite', 'leak', 'eau', 'water', 'condensation']):
            return "üíß Probl√®me d'humidit√© d√©tect√©.\n\nSolutions:\n1. Nettoyer l'√©vacuation\n2. V√©rifier les joints\n3. V√©rifier l'√©vaporateur\n4. Contr√¥ler le drainage"
        
        elif any(word in message_lower for word in ['diagnostique', 'diagnostic', 'analyse', 'analyze', 'test']):
            return "üîç Diagnostic en cours.\n\nParam√®tres v√©rifi√©s:\n- Temp√©rature interne/externe\n- Pression du circuit\n- Consommation √©lectrique\n- Cycles de compresseur\n- √âtat des alarmes"
        
        elif intent == 'solution':
            return "üîß D√©pannage recommand√©:\n1. Arr√™ter et red√©marrer\n2. V√©rifier tous les raccordements\n3. Nettoyer les surfaces\n4. Tester en mode diagnostic\n5. Consulter la documentation"
        
        elif intent == 'alert':
            return "üö® Alerte syst√®me d√©tect√©e et enregistr√©e.\n\nLe diagnostic proc√®de √† l'analyse des anomalies. Veuillez consulter le tableau de bord pour les d√©tails."
        
        else:
            # Fallback g√©n√©rique
            return f"üí¨ Traitement de votre demande relative aux syst√®mes frigorifiques.\n\nPour plus d'informations sp√©cifiques, veuillez:\n1. D√©crire le sympt√¥me observ√©\n2. Indiquer la temp√©rature actuelle\n3. Signaler toute anomalie"
    
    def _build_prompt(self, message, context, intent):
        """Construire le prompt pour le mod√®le - COURT et STRUCTUR√â"""
        # Prompt court mais structur√© pour √©viter du bruit
        prompt = f"""Vous etes un expert en diagnostic frigorifique. Repondez brievement.

Question: {message}

Reponse courte et technique:"""
        return prompt
    
    def _analyze_alert_severity(self, alert_data):
        """Analyser la s√©v√©rit√© d'une alerte"""
        severity_map = {
            'info': 1,
            'warning': 2,
            'error': 3,
            'critical': 4
        }
        base_severity = severity_map.get(alert_data.get('severity', 'info'), 1)
        
        # Ajuster selon les donn√©es
        if 'temperature' in str(alert_data).lower():
            base_severity *= 1.2
        
        return min(base_severity, 4)
    
    def _find_solutions(self, alert_data):
        """Trouver des solutions pour une alerte"""
        solutions = []
        alert_str = str(alert_data).lower()
        
        # Solutions pr√©-d√©finies
        solution_map = {
            'temperature': [
                'V√©rifier le thermostat',
                'Nettoyer les filtres',
                'V√©rifier la circulation d\'air'
            ],
            'pressure': [
                'V√©rifier le compresseur',
                'V√©rifier les connexions',
                'Mesurer la pression'
            ],
            'noise': [
                'V√©rifier les vibrations',
                'V√©rifier le compresseur',
                'V√©rifier le ventilateur'
            ]
        }
        
        for key, sols in solution_map.items():
            if key in alert_str:
                solutions.extend(sols)
        
        return solutions[:3]  # Top 3 solutions
    
    def _save_to_history(self, user_id, message, response):
        """Sauvegarder dans l'historique"""
        entry = {
            'user_id': user_id,
            'message': message,
            'response': response,
            'timestamp': datetime.now().isoformat()
        }
        self.conversation_history.append(entry)
        
        # Garder seulement les N derniers messages
        if len(self.conversation_history) > 100:
            self.conversation_history = self.conversation_history[-100:]
    
    def add_to_knowledge_base(self, topic, content):
        """Ajouter une entr√©e √† la base de connaissances"""
        self.knowledge_base[topic] = content
        
        # Sauvegarder
        kb_file = self.config.DB_PATH / "knowledge_base.json"
        with open(kb_file, 'w', encoding='utf-8') as f:
            json.dump(self.knowledge_base, f, ensure_ascii=False, indent=2)
        
        logger.info(f"‚úÖ Entr√©e ajout√©e √† la KB: {topic}")
    
    def get_stats(self):
        """Obtenir les statistiques du service"""
        return {
            'model': self.model_name,
            'messages_processed': len(self.conversation_history),
            'knowledge_base_size': len(self.knowledge_base),
            'uptime': datetime.now().isoformat()
        }


# Singleton global
_ia_service = None

def get_ia_service(model='phi'):
    """Obtenir l'instance du service IA (singleton)"""
    global _ia_service
    if _ia_service is None:
        _ia_service = IAService(model)
    return _ia_service
