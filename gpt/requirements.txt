# Service IA Local - Dépendances

# Framework Web
Flask==3.0.0
flask-cors==4.0.0

# Modèles LLM (versions stables pré-compilées)
torch==2.5.1
transformers==4.41.0
accelerate==0.29.0

# Optimisation modèles
bitsandbytes==0.43.0
peft==0.11.0

# Utilities
python-dotenv==1.0.0
requests==2.31.0

# Logging
python-json-logger==2.0.7

# Production
gunicorn==21.2.0
gevent==23.9.1

# Testing
pytest==7.4.3

# Note: Modèles utilisés (ajuster selon disponibilité):
# - Microsoft/phi-2 (2.7B - TRÈS LÉGER, RECOMMANDÉ POUR COMMENCER)
# - mistralai/Mistral-7B-Instruct-v0.1 (7B)
# - meta-llama/Llama-2-7b-chat (7B - nécessite accès HF)
# - gpt2 (simple, léger, entraînement de base)

# CONFIGURATION MODÈLES LOCAUX:
# Les modèles sont stockés dans le dossier ../models/{model_name}
# Le service IA charge par défaut depuis ce chemin (pas de téléchargement HF)
# Priorité de chargement:
#   1. Variable d'environnement HF_LOCAL_MODEL_PATH (si définie)
#   2. Dossier ../models/{model_name} (chemin par défaut)
#   3. Téléchargement depuis HuggingFace (si pas d'accès local et connexion disponible)
#
# Pour ajouter des modèles locaux:
#   - Placer le dossier du modèle dans ../models/{model_name}
#   - Ou définir HF_LOCAL_MODEL_PATH avant de lancer app_ia.py
