#!/usr/bin/env python3
"""
Script de test pour v√©rifier le chargement du mod√®le LLM
Teste Phi-2 et les autres mod√®les disponibles
"""

import os
import sys
import logging
import torch

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def test_device():
    """Tester la disponibilit√© du GPU"""
    logger.info("üñ• V√©rification des ressources...")
    logger.info(f"  GPU disponible: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        logger.info(f"  GPU utilis√©: {torch.cuda.get_device_name(0)}")
        logger.info(f"  VRAM disponible: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    logger.info(f"  CPU: {os.cpu_count()} cores")

def test_imports():
    """Tester l'import des d√©pendances"""
    logger.info("üì¶ V√©rification des d√©pendances...")
    
    try:
        import torch
        logger.info("  ‚úÖ torch")
    except ImportError as e:
        logger.error(f"  ‚ùå torch: {e}")
        return False
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
        logger.info("  ‚úÖ transformers")
    except ImportError as e:
        logger.error(f"  ‚ùå transformers: {e}")
        return False
    
    try:
        import flask
        logger.info("  ‚úÖ flask")
    except ImportError as e:
        logger.error(f"  ‚ùå flask: {e}")
        return False
    
    return True

def test_phi2_loading():
    """Tester le chargement de Phi-2"""
    logger.info("\nüöÄ Test de chargement du mod√®le Phi-2...")
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        model_id = "microsoft/phi-2"
        logger.info(f"  Mod√®le: {model_id}")
        logger.info(f"  Taille: 2.7B")
        logger.info("  ‚è≥ Chargement du tokenizer...")
        
        tokenizer = AutoTokenizer.from_pretrained(
            model_id,
            trust_remote_code=True,
        )
        logger.info("  ‚úÖ Tokenizer charg√©")
        
        logger.info("  ‚è≥ Chargement du mod√®le (peut prendre 1-2 minutes)...")
        
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            trust_remote_code=True,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
        )
        logger.info("  ‚úÖ Mod√®le charg√©")
        
        # Test de g√©n√©ration simple
        logger.info("  ‚è≥ Test de g√©n√©ration...")
        model.eval()
        
        prompt = "Diagnostic frigorifique: Le compresseur ne d√©marre pas. "
        inputs = tokenizer(prompt, return_tensors="pt")
        
        # G√©n√©rer avec les param√®tres standard
        with torch.no_grad():
            outputs = model.generate(
                inputs['input_ids'],
                max_new_tokens=100,
                temperature=0.7,
                top_p=0.95,
                do_sample=True,
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        logger.info(f"  ‚úÖ G√©n√©ration r√©ussie")
        logger.info(f"  üìù Prompt: {prompt}")
        logger.info(f"  üí¨ R√©ponse: {response[len(prompt):].strip()[:100]}...")
        
        return True
        
    except Exception as e:
        logger.error(f"  ‚ùå Erreur: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_ia_service():
    """Tester le service IA"""
    logger.info("\nü§ñ Test du service IA...")
    
    try:
        from ia_service import IAService
        
        logger.info("  ‚è≥ Initialisation du service IA...")
        service = IAService(model_name='phi')
        
        logger.info("  ‚úÖ Service IA initialis√©")
        
        # Test de traitement de message
        logger.info("  ‚è≥ Test de traitement d'un message...")
        result = service.process_chat_message(
            "Le compresseur ne d√©marre pas, qu'est-ce que je dois faire?",
            user_id="test_user"
        )
        
        if result['success']:
            logger.info("  ‚úÖ Message trait√©")
            logger.info(f"  Intent d√©tect√©: {result['intent']}")
            logger.info(f"  R√©ponse: {result['response'][:100]}...")
        else:
            logger.error(f"  ‚ùå Erreur: {result.get('error', 'Inconnu')}")
            return False
        
        return True
        
    except Exception as e:
        logger.error(f"  ‚ùå Erreur: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Fonction principale"""
    logger.info("=" * 60)
    logger.info("üß™ TEST DU SERVICE IA - MOD√àLE PHI-2")
    logger.info("=" * 60)
    
    # Test 1: Ressources
    test_device()
    
    # Test 2: D√©pendances
    if not test_imports():
        logger.error("\n‚ùå D√©pendances manquantes. Installer avec:")
        logger.error("pip install -r requirements.txt")
        return 1
    
    # Test 3: Chargement du mod√®le
    if not test_phi2_loading():
        logger.warning("\n‚ö† Le chargement du mod√®le a √©chou√©")
        logger.warning("V√©rifier la connexion internet et l'espace disque disponible")
    
    # Test 4: Service IA
    if not test_ia_service():
        logger.warning("\n‚ö† Le test du service IA a √©chou√©")
    
    logger.info("\n" + "=" * 60)
    logger.info("‚úÖ Tests compl√©t√©s!")
    logger.info("=" * 60)
    
    return 0

if __name__ == '__main__':
    sys.exit(main())
