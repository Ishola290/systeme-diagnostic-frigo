# ü§ñ Service IA Local - Guide Complet

## üìã Vue d'ensemble

Le **Service IA Local** est une couche IA centralis√©e qui remplace Gemini et Telegram par des **mod√®les open-source locaux**.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   Architecture                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ  ‚îÇ   App.py       ‚îÇ         ‚îÇ  Chat Web    ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ  (5000)        ‚îÇ         ‚îÇ  (5001)      ‚îÇ           ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îÇ         ‚îÇ Alertes                   ‚îÇ Messages          ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ
‚îÇ                        ‚îÇ                                ‚îÇ
‚îÇ                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                       ‚îÇ
‚îÇ                   ‚îÇ Service IA ‚îÇ                       ‚îÇ
‚îÇ                   ‚îÇ   (5002)   ‚îÇ                       ‚îÇ
‚îÇ                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                       ‚îÇ
‚îÇ                         ‚îÇ                              ‚îÇ
‚îÇ          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ          ‚îÇ              ‚îÇ              ‚îÇ              ‚îÇ
‚îÇ       ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ       ‚îÇ  LLM‚îÇ    ‚îÇ Knowledge  ‚îÇ   ‚îÇ Alerte ‚îÇ        ‚îÇ
‚îÇ       ‚îÇModel‚îÇ    ‚îÇ    Base    ‚îÇ   ‚îÇProcess ‚îÇ        ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îÇ                                                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üöÄ D√©marrage Rapide

### 1. Installation des d√©pendances

```bash
cd gpt
pip install -r requirements.txt
```

### 2. D√©marrer le service

```bash
python app_ia.py
```

**Sortie attendue:**
```
‚úÖ Service IA initialis√©
üöÄ D√©marrage du service IA local
üìù Endpoints disponibles:
   POST /api/chat/message - Traiter message chat
   ...
```

### 3. Test rapide

```bash
curl -X POST http://localhost:5002/api/chat/message \
  -H "Content-Type: application/json" \
  -d '{"message": "Diagnostic: temp√©rature √©lev√©e, bruit", "user_id": "test"}'
```

---

## üìä Architecture des Fichiers

```
gpt/
‚îú‚îÄ‚îÄ app_ia.py              # API Flask du service IA
‚îú‚îÄ‚îÄ ia_service.py          # Logique du service IA
‚îú‚îÄ‚îÄ config.py              # Configuration
‚îú‚îÄ‚îÄ __init__.py            # Package init
‚îú‚îÄ‚îÄ requirements.txt       # D√©pendances Python
‚îú‚îÄ‚îÄ data/                  # Donn√©es persistantes
‚îÇ   ‚îú‚îÄ‚îÄ knowledge_base.json # Base de connaissances
‚îÇ   ‚îî‚îÄ‚îÄ models/            # Mod√®les t√©l√©charg√©s
‚îú‚îÄ‚îÄ cache/                 # Cache des inf√©rences
‚îú‚îÄ‚îÄ logs/                  # Fichiers de log
‚îî‚îÄ‚îÄ tests/                 # Tests unitaires (futur)
```

---

## üß† Mod√®les Disponibles

| Mod√®le | Taille | Vitesse | Qualit√© | VRAM | Status |
|--------|--------|---------|---------|------|--------|
| **Phi-2** | 2.7B | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | 4GB | ‚úÖ RECOMMAND√â |
| **Mistral-7B** | 7B | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | 8GB | ‚úÖ BON |
| **Neural Chat** | 7B | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | 8GB | ‚úÖ BON |
| **Llama-2-7B** | 7B | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 16GB | ‚ö†Ô∏è LOURD |
| **GPT-2** | 124M | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | 1GB | ‚úÖ TEST |

### Recommandation

**D√©marrer avec Phi-2:**
```bash
export IA_MODEL=phi
python app_ia.py
```

---

## üîå API Endpoints

### 1. Traiter un message du chat

```http
POST /api/chat/message
Content-Type: application/json

{
  "message": "Diagnostic: temp√©rature √©lev√©e, bruit",
  "user_id": "user123"
}

Response:
{
  "success": true,
  "response": "üîç Diagnostic en cours...",
  "intent": "diagnostic",
  "timestamp": "2025-11-20T..."
}
```

### 2. Traiter une alerte

```http
POST /api/alerts/process
Content-Type: application/json

{
  "type": "error",
  "title": "Temp√©rature trop √©lev√©e",
  "message": "La temp√©rature a atteint 35¬∞C",
  "severity": "critical",
  "diagnostic_id": "diag_123"
}

Response:
{
  "success": true,
  "alert": {
    "...": "...",
    "processed": true,
    "severity_score": 3.8,
    "suggested_solutions": [
      "V√©rifier le thermostat",
      "Nettoyer les filtres",
      "..."
    ]
  }
}
```

### 3. Ajouter √† la base de connaissances

```http
POST /api/knowledge/add
Content-Type: application/json

{
  "topic": "temp√©rature_√©lev√©e",
  "content": {
    "cause": "Thermostat d√©faillant",
    "solution": "Remplacer le thermostat",
    "confidence": 0.95
  }
}

Response:
{
  "success": true,
  "message": "Entr√©e \"temp√©rature_√©lev√©e\" ajout√©e"
}
```

### 4. Analyser un diagnostic

```http
POST /api/diagnostic/analyze
Content-Type: application/json

{
  "symptoms": ["temp√©rature √©lev√©e", "bruit anormal"],
  "measurements": {
    "temperature": 35,
    "pressure_hp": 18,
    "pressure_bp": 2
  }
}

Response:
{
  "success": true,
  "diagnosis": "Probable d√©faut du thermostat",
  "symptoms": ["temp√©rature √©lev√©e", "bruit anormal"],
  "intent": "diagnostic"
}
```

### 5. Apprentissage

```http
POST /api/learn
Content-Type: application/json

{
  "case": "temp√©rature basse + silence complet",
  "solution": "Compresseur arr√™t√© - V√©rifier alimentation",
  "confidence": 0.9
}

Response:
{
  "success": true,
  "message": "Cas d'apprentissage enregistr√©"
}
```

### 6. Statistiques

```http
GET /api/stats

Response:
{
  "model": "phi",
  "messages_processed": 156,
  "knowledge_base_size": 42,
  "uptime": "2025-11-20T..."
}
```

### 7. Mod√®les disponibles

```http
GET /api/models

Response:
{
  "available_models": {
    "mistral": "mistralai/Mistral-7B-Instruct-v0.1",
    "phi": "Microsoft/phi-2",
    ...
  },
  "current_model": "phi"
}
```

---

## üîó Int√©gration avec les autres services

### From App.py (Alertes)

```python
import requests

# Au lieu d'appeler Gemini directement
# Envoyer √† notre service IA
alert_data = {
    'type': 'error',
    'title': 'Temp√©rature √©lev√©e',
    'message': 'T > 35¬∞C',
    'severity': 'critical'
}

response = requests.post(
    'http://localhost:5002/api/alerts/process',
    json=alert_data
)

processed_alert = response.json()['alert']

# Envoyer l'alerte trait√©e au chat
requests.post(
    'http://localhost:5001/api/alerts',
    json=processed_alert
)
```

### From Chat Web (Messages)

```javascript
// Au lieu d'appeler Gemini directement
// Envoyer √† notre service IA
const response = await fetch('http://localhost:5002/api/chat/message', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
        message: "Diagnostic: temp√©rature √©lev√©e",
        user_id: current_user.id
    })
});

const data = await response.json();
console.log('R√©ponse IA:', data.response);
```

---

## üíæ Base de Connaissances

La base de connaissances est stock√©e dans `data/knowledge_base.json`:

```json
{
  "temp√©rature_√©lev√©e": {
    "cause": "Thermostat d√©faillant ou compresseur surcharg√©",
    "solutions": [
      "V√©rifier le thermostat",
      "Nettoyer les filtres",
      "V√©rifier la circulation d'air"
    ],
    "confidence": 0.95
  },
  "bruit_anormal": {
    "cause": "Compresseur d√©faillant ou vibrations",
    "solutions": [
      "V√©rifier le compresseur",
      "V√©rifier les amortisseurs"
    ],
    "confidence": 0.88
  }
}
```

### Ajouter des entr√©es (√† faire dans app.py):

```python
requests.post('http://localhost:5002/api/knowledge/add', json={
    'topic': 'erreur_e02',
    'content': {
        'description': 'Erreur capteur temp√©rature',
        'solutions': ['Remplacer le capteur', 'V√©rifier le c√¢blage'],
        'severity': 'high'
    }
})
```

---

## üê≥ Docker

### Build

```bash
docker build -t frigo-ia .
```

### Run

```bash
docker run -p 5002:5002 \
  -e IA_MODEL=phi \
  -e IA_USE_GPU=true \
  frigo-ia
```

### Docker Compose

```yaml
ia-service:
  build:
    context: .
    dockerfile: Dockerfile
  ports:
    - "5002:5002"
  environment:
    - IA_MODEL=phi
    - IA_USE_GPU=true
  volumes:
    - ia-data:/app/data
    - ia-cache:/app/cache
    - ia-models:/app/models
```

---

## üîß Configuration

Via les variables d'environnement:

```bash
# Mod√®le
export IA_MODEL=phi  # ou mistral, neural, llama2, gpt2

# Performance
export IA_USE_GPU=true
export IA_QUANTIZE=true

# Param√®tres LLM
export IA_MAX_TOKENS=512
export IA_TEMPERATURE=0.7

# URLs
export MAIN_API_URL=http://localhost:5000
export CHAT_API_URL=http://localhost:5001
```

---

## üìä Performance

### Benchmark (Test local)

| Mod√®le | Load Time | Inf√©rence | VRAM |
|--------|-----------|-----------|------|
| Phi-2 | 3s | 100ms/token | 4GB |
| Mistral-7B | 8s | 80ms/token | 8GB |
| Neural-7B | 8s | 85ms/token | 8GB |
| GPT-2 | 1s | 50ms/token | 1GB |

---

## üß™ Tests

```bash
# Test de sant√©
curl http://localhost:5002/health

# Test message
curl -X POST http://localhost:5002/api/chat/message \
  -H "Content-Type: application/json" \
  -d '{"message":"Bonjour","user_id":"test"}'

# Test alerte
curl -X POST http://localhost:5002/api/alerts/process \
  -H "Content-Type: application/json" \
  -d '{"title":"Test","severity":"info"}'
```

---

## üöÄ Prochaines √âtapes

1. **Fine-tuning** - Adapter le mod√®le aux diagnostics frigo
2. **RAG (Retrieval)** - Int√©grer recherche s√©mantique dans KB
3. **Monitoring** - Ajouter Prometheus/Grafana
4. **Caching** - Optimiser les inf√©rences fr√©quentes
5. **Multi-GPU** - Support de plusieurs GPUs
6. **Distillation** - Cr√©er mod√®les plus petits

---

## üìû D√©pannage

### Le service ne d√©marre pas

```bash
# V√©rifier les logs
python app_ia.py

# V√©rifier les d√©pendances
pip install -r requirements.txt

# V√©rifier CUDA (si GPU)
python -c "import torch; print(torch.cuda.is_available())"
```

### Erreur CUDA out of memory

```bash
# R√©duire la taille du mod√®le
export IA_MODEL=phi

# Ou d√©sactiver GPU
export IA_USE_GPU=false
```

### Service trop lent

```bash
# Activer la quantification
export IA_QUANTIZE=true

# R√©duire MAX_TOKENS
export IA_MAX_TOKENS=256
```

---

## ‚ú® Points Forts

‚úÖ **Open-source** - Mod√®les libres et personnalisables  
‚úÖ **Local** - Pas d'appels API externes  
‚úÖ **Rapide** - Inf√©rence < 200ms  
‚úÖ **Flexible** - Changement facile de mod√®les  
‚úÖ **√âconomique** - Pas de co√ªts API  
‚úÖ **Priv√©** - Donn√©es locales  

---

**Pr√™t √† d√©ployer!** üöÄ
