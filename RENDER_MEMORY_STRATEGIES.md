# ğŸ§  StratÃ©gies MÃ©moire pour Render - Alternatives ComplÃ¨tes

## âš ï¸ Le ProblÃ¨me

| Service | RAM Requise | Render Gratuit | Render Standard | Render Premium |
|---------|-------------|---|---|---|
| **phi-2 (5GB)** | 8-12 GB | âŒ Crash | âš ï¸ Lent | âœ… OK |
| **gpt2 (500MB)** | 2-3 GB | âœ… OK | âœ… OK | âœ… OK |
| **mistral-7B (13GB)** | 16+ GB | âŒ Crash | âŒ Crash | âš ï¸ Lent |

**Render Gratuit**: 512MB RAM (trop petit)
**Render Standard**: 1-2GB RAM (insuffisant pour phi)
**Render Premium**: 4GB+ RAM (fonctionne, coÃ»teux)

---

## ğŸ¯ StratÃ©gie 1: ModÃ¨le LÃ©ger (RecommandÃ© - Gratuit)

### **Remplacer phi-2 par gpt2**

#### Avantages
- âœ… **RAM**: 2-3GB (fonctionne sur instance gratuite)
- âœ… **Speed**: DÃ©marrage <2 sec
- âœ… **CoÃ»t**: Gratuit
- âœ… **Stockage**: 500MB (tÃ©lÃ©charge rapidement)

#### InconvÃ©nients
- âš ï¸ **QualitÃ©**: Moins bon que phi-2 (mais convenable pour diag frigo)
- âš ï¸ **Performance**: Plus lent en infÃ©rence

#### ImplÃ©mentation

**Fichier: `Dockerfile` (racine)**
```dockerfile
# âŒ Ancien (trop lourd)
RUN python download_models.py --model phi

# âœ… Nouveau (lÃ©ger)
RUN python download_models.py --model gpt2
```

**Fichier: `gpt/Dockerfile`**
```dockerfile
# âŒ Ancien
RUN python download_models.py --model phi

# âœ… Nouveau
RUN python download_models.py --model gpt2
```

**Fichier: `download_models.py`**
```python
# âœ… Mettre gpt2 par dÃ©faut
DEFAULT_MODEL = 'gpt2'  # Au lieu de 'phi'
```

**Fichier: `.env.production` (Render)**
```env
IA_MODEL=gpt2           # Changer de phi
HF_LOCAL_MODEL_PATH=/app/models/gpt2
```

#### RÃ©sultat
- **Build time**: 3-5 min (ultra rapide)
- **Image size**: 1-1.5GB (fit dans instance gratuite)
- **RAM usage**: 500MB-1GB (OK)
- **CoÃ»t**: 0â‚¬/mois

---

## ğŸš€ StratÃ©gie 2: ModÃ¨le Dynamique sur HuggingFace (RecommandÃ© - Scalable)

### **TÃ©lÃ©charger le modÃ¨le Ã  la demande (pas prÃ©-compilÃ©)**

#### Concept
```
Client â†’ API â†’ "Model not loaded"
              â†’ Download from HuggingFace (~1 min)
              â†’ Cache localement
              â†’ RÃ©pondre
```

#### Avantages
- âœ… **Image lÃ©gÃ¨re**: 500MB (juste app + deps)
- âœ… **Gratuit**: Aucun frais supplÃ©mentaire
- âœ… **Flexible**: Changer de modÃ¨le sans redÃ©ployer
- âœ… **Scalable**: Marche sur instance petite

#### InconvÃ©nients
- âš ï¸ **Premier appel**: 30-60 sec (tÃ©lÃ©charge modÃ¨le)
- âš ï¸ **Stockage**: Epuise disque Render aprÃ¨s plusieurs requÃªtes

#### ImplÃ©mentation

**Fichier: `gpt/ia_service.py` (modifier la fonction d'init)**

```python
import os
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

class ModelManager:
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance.model = None
            cls._instance.tokenizer = None
        return cls._instance
    
    def load_model(self, model_name='gpt2'):
        """Charge le modÃ¨le avec caching local"""
        
        # Chemin local (Render a /app et /tmp)
        local_path = f"/app/models/{model_name}"
        
        # PrioritÃ© 1: ModÃ¨le prÃ©-compilÃ© (s'il existe)
        if os.path.exists(local_path):
            print(f"âœ… Chargement depuis cache local: {local_path}")
            self.model = AutoModelForCausalLM.from_pretrained(local_path)
            self.tokenizer = AutoTokenizer.from_pretrained(local_path)
            return
        
        # PrioritÃ© 2: TÃ©lÃ©charger de HuggingFace
        print(f"ğŸ“¥ TÃ©lÃ©chargement de {model_name} depuis HuggingFace...")
        try:
            # TÃ©lÃ©charger avec cache dans /tmp (disque Render)
            cache_dir = "/tmp/hf_cache"
            os.makedirs(cache_dir, exist_ok=True)
            
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                cache_dir=cache_dir,
                device_map="auto"
            )
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                cache_dir=cache_dir
            )
            print(f"âœ… {model_name} chargÃ© depuis HuggingFace")
        except Exception as e:
            print(f"âŒ Erreur: {e}")
            # Fallback vers gpt2
            self.load_model('gpt2')
    
    def infer(self, prompt, max_length=100):
        """InfÃ©rence avec lazy-loading"""
        if self.model is None:
            self.load_model()
        
        inputs = self.tokenizer.encode(prompt, return_tensors='pt')
        outputs = self.model.generate(inputs, max_length=max_length)
        return self.tokenizer.decode(outputs[0])

# Usage dans app_ia.py
model_manager = ModelManager()

@app.route('/api/infer', methods=['POST'])
def infer():
    data = request.json
    prompt = data.get('prompt', '')
    result = model_manager.infer(prompt)
    return {'result': result}
```

**Dockerfile - Version LÃ©gÃ¨re (Render)**

```dockerfile
FROM python:3.11-slim

WORKDIR /app

RUN apt-get update && apt-get install -y \
    gcc curl \
    && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

RUN mkdir -p /app/logs /app/cache

ENV FLASK_ENV=production
ENV PYTHONUNBUFFERED=1
ENV HF_HOME=/tmp/hf_cache

EXPOSE 5002

CMD ["python", "app_ia.py"]
```

#### RÃ©sultat
- **Build time**: 2-3 min (ultra rapide)
- **Image size**: 500MB
- **Premier appel**: +30-60 sec (tÃ©lÃ©charge modÃ¨le)
- **Appels suivants**: InstantanÃ©
- **CoÃ»t**: 0â‚¬/mois

---

## ğŸ’¾ StratÃ©gie 3: AWS S3 / Google Cloud Storage (RecommandÃ© - Pro)

### **ModÃ¨le hÃ©bergÃ© externement, tÃ©lÃ©chargÃ© Ã  l'init**

#### Concept
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Render      â”‚
â”‚  (app)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â””â”€â”€â†’ S3 / GCS
             (modÃ¨le stockÃ©)
             â”‚
             â””â”€â”€â†’ TÃ©lÃ©charge au dÃ©marrage
                  (cache local)
```

#### Avantages
- âœ… **Image compacte**: 200MB (sans modÃ¨le)
- âœ… **Rapide**: TÃ©lÃ©charge au dÃ©marrage (~30 sec avec CDN)
- âœ… **Flexible**: Changer de modÃ¨le sans redÃ©ployer code
- âœ… **Versioning**: GÃ©rer plusieurs versions

#### InconvÃ©nients
- ğŸ’° **CoÃ»t**: ~$0.10-1/mois (stockage S3)
- ğŸ”§ **Setup**: Configuration AWS/GCS requise
- ğŸŒ **RÃ©seau**: DÃ©pend de connexion cloud

#### ImplÃ©mentation (AWS S3)

**1. Upload modÃ¨le sur S3**
```powershell
# Local (ta machine)
aws s3 cp models/phi-2/ s3://frigo-models/phi-2/ --recursive

# CoÃ»t: ~$0.10/mois pour 5GB
```

**2. Ajouter boto3 dans requirements.txt**
```
boto3==1.28.85
```

**3. Modifier `gpt/ia_service.py`**
```python
import boto3
from botocore.exceptions import NoCredentialsError

def download_model_from_s3(model_name):
    """TÃ©lÃ©charge modÃ¨le depuis S3 au dÃ©marrage"""
    
    s3 = boto3.client('s3')
    bucket = 'frigo-models'
    local_path = f'/app/models/{model_name}'
    
    try:
        # TÃ©lÃ©charger tous les fichiers
        paginator = s3.get_paginator('list_objects_v2')
        pages = paginator.paginate(Bucket=bucket, Prefix=f'{model_name}/')
        
        for page in pages:
            for obj in page.get('Contents', []):
                key = obj['Key']
                if key.endswith('/'):
                    os.makedirs(f'/app/{key}', exist_ok=True)
                else:
                    local_file = f'/app/{key}'
                    os.makedirs(os.path.dirname(local_file), exist_ok=True)
                    print(f"ğŸ“¥ TÃ©lÃ©chargement: {key}...")
                    s3.download_file(bucket, key, local_file)
        
        print(f"âœ… ModÃ¨le {model_name} tÃ©lÃ©chargÃ© depuis S3")
    except NoCredentialsError:
        print("âŒ AWS credentials manquantes")
        raise

# Appeler au dÃ©marrage app_ia.py
@app.before_first_request
def init_model():
    download_model_from_s3('phi-2')
    model_manager.load_model('phi-2')
```

**4. Variables d'env Render**
```env
AWS_ACCESS_KEY_ID=xxxxx
AWS_SECRET_ACCESS_KEY=xxxxx
AWS_DEFAULT_REGION=eu-west-1
```

#### RÃ©sultat
- **Build time**: 2 min
- **Image size**: 200MB
- **Startup time**: 30-60 sec (tÃ©lÃ©charge S3)
- **CoÃ»t**: 0â‚¬ (Render) + ~$0.10/mois (S3)

---

## ğŸŒ StratÃ©gie 4: API Externe (RecommandÃ© - Basique)

### **Utiliser une API de modÃ¨les prÃ©-dÃ©ployÃ©e**

#### Concept
```
Ton app â†’ Appelle API â†’ HuggingFace Inference API
                        ou Replicate
                        ou Together AI
```

#### Avantages
- âœ… **ZÃ©ro serveur**: Pas besoin d'hÃ©berger modÃ¨le
- âœ… **Image minuscule**: 100MB
- âœ… **Gratuit**: Tier gratuit disponible (limitÃ©)
- âœ… **Simple**: Juste appels HTTP

#### InconvÃ©nients
- ğŸ’° **CoÃ»t**: ~$0.001-0.01 par requÃªte
- ğŸŒ **DÃ©pendance**: Reliant Ã  service externe
- â±ï¸ **Latence**: 2-10 sec par appel (rÃ©seau)

#### ImplÃ©mentation (HuggingFace Inference API)

**1. Obtenir API token**
```
https://huggingface.co/settings/tokens
â†’ New token (Read) â†’ copier
```

**2. Modifier `gpt/app_ia.py`**
```python
import requests

HF_API_TOKEN = os.getenv('HF_API_TOKEN')
HF_MODEL_ID = 'microsoft/phi-2'
HF_API_URL = f'https://api-inference.huggingface.co/models/{HF_MODEL_ID}'

@app.route('/api/infer', methods=['POST'])
def infer():
    data = request.json
    prompt = data.get('prompt', '')
    
    headers = {'Authorization': f'Bearer {HF_API_TOKEN}'}
    response = requests.post(
        HF_API_URL,
        headers=headers,
        json={'inputs': prompt}
    )
    
    if response.status_code == 200:
        result = response.json()[0]['generated_text']
        return {'result': result}
    else:
        return {'error': response.text}, 500
```

**3. Variables d'env Render**
```env
HF_API_TOKEN=hf_xxxxxxxxxxxxx
```

#### RÃ©sultat
- **Build time**: 1-2 min (ultra rapide)
- **Image size**: 100MB
- **InfÃ©rence**: 2-10 sec (rÃ©seau)
- **CoÃ»t**: ~$0.01-1/mois (selon usage)

---

## ğŸ“Š Comparaison ComplÃ¨te

| StratÃ©gie | RAM | Disque | Build | CoÃ»t | Latence | QualitÃ© |
|-----------|-----|--------|-------|------|---------|---------|
| **1. gpt2 (LÃ©ger)** | 512MB âœ… | 500MB | 3min | 0â‚¬ | 100ms | Bonne |
| **2. HF Dynamic** | 512MB âœ… | 1GB | 2min | 0â‚¬ | 1s (1st), 100ms | Excellente |
| **3. S3 Download** | 512MB âœ… | 2GB | 2min | $0.1 | 100ms | Excellente |
| **4. HF API** | 256MB âœ… | 100MB | 1min | $0.01-1 | 2-10s | Excellente |
| **phi-2 (PrÃ©-compilÃ©)** | 8GB âŒ | 6GB âŒ | 20min | 0â‚¬ | 100ms | Excellent |

---

## ğŸ¯ Recommandations par Cas

### **Budget: ZÃ©ro, RapiditÃ©: Important**
ğŸ‘‰ **StratÃ©gie 2 (HF Dynamic)** 
- TÃ©lÃ©charge modÃ¨le Ã  la demande
- Premier appel: 30-60 sec
- Appels suivants: rapide

### **Budget: ZÃ©ro, Latence: Critique**
ğŸ‘‰ **StratÃ©gie 1 (gpt2 LÃ©ger)**
- Utilise gpt2 prÃ©-compilÃ©
- InstantanÃ© toujours
- QualitÃ© acceptable pour diagnostics frigo

### **Budget: Petit ($10/mois)**
ğŸ‘‰ **StratÃ©gie 3 (S3)**
- ModÃ¨le sur AWS S3
- TÃ©lÃ©charge Ã  l'init (~30 sec)
- Rapide aprÃ¨s

### **Budget: Flexible**
ğŸ‘‰ **StratÃ©gie 4 (API Externe)**
- ZÃ©ro infra
- Pay-as-you-go
- Latence acceptable (2-10s)

---

## ğŸ”§ ImplÃ©mentation RecommandÃ©e (Hybride)

### **Approche Intelligent Fallback**

```python
# gpt/app_ia.py
class SmartModelLoader:
    """Charge modÃ¨le avec stratÃ©gie intelligente"""
    
    @staticmethod
    def load():
        """PrioritÃ©s: Local â†’ S3 â†’ HF Dynamic â†’ API â†’ gpt2"""
        
        # PrioritÃ© 1: ModÃ¨le prÃ©-compilÃ© local
        if os.path.exists('/app/models/phi'):
            return load_local_model('phi')
        
        # PrioritÃ© 2: S3 (si credentials disponibles)
        if os.getenv('AWS_ACCESS_KEY_ID'):
            return load_from_s3('phi-2')
        
        # PrioritÃ© 3: HuggingFace Dynamic
        if os.getenv('INTERNET_AVAILABLE'):
            return load_from_huggingface('phi-2')
        
        # PrioritÃ© 4: API Externe (HF Inference)
        if os.getenv('HF_API_TOKEN'):
            return HuggingFaceAPIClient('phi-2')
        
        # Fallback: gpt2 local (toujours dispo)
        return load_local_model('gpt2')
```

#### Deployment Flow
```
Production (Render):
1. Essaie charger /app/models/phi (stratÃ©gie 1 ou 3)
2. Si pas dispo, tÃ©lÃ©charge de HF (stratÃ©gie 2)
3. Si connexion lente, utilise API (stratÃ©gie 4)
4. Fallback: gpt2 (stratÃ©gie 1)
```

---

## ğŸ“‹ Checklist DÃ©ploiement Render

### **Avant de Pousser sur Render**

- [ ] Choisir stratÃ©gie (1-4 ci-dessus)
- [ ] Modifier `Dockerfile` pour stratÃ©gie choisie
- [ ] Tester localement: `docker build .`
- [ ] Git push les changes
- [ ] Configurer variables d'env sur Render

### **Pendant le DÃ©ploiement**

- [ ] Render commence le build
- [ ] Monitor les logs de compilation
- [ ] Attendre selon timing (3min-20min)
- [ ] VÃ©rifier que service dÃ©marre

### **AprÃ¨s le DÃ©ploiement**

- [ ] Test: `GET /health`
- [ ] Test: `POST /api/infer` (mesurer temps)
- [ ] Monitor: Render dashboard (RAM/CPU)
- [ ] Optimiser: RÃ©ajuster si crashes

---

## ğŸš¨ Si le DÃ©ploiement Ã‰choue

### **Erreur: "Out of Memory"**
```
Solution rapide:
1. Basculer Ã  stratÃ©gie gpt2 (lÃ©ger)
2. RedÃ©ployer
3. Plus tard: Upgrade instance ou S3
```

### **Erreur: "Disk space full"**
```
Solution:
1. RÃ©duire taille modÃ¨le (gpt2 au lieu phi)
2. Utiliser S3 (tÃ©lÃ©chargement Ã  l'init)
3. Ou: Upgrade instance Render
```

### **Service dÃ©marre mais trÃ¨s lent**
```
ProblÃ¨me: ModÃ¨le chargÃ© en RAM, pas assez
Solutions:
1. Utiliser quantization (4-bit)
2. Utiliser ONNX format (plus rapide)
3. RÃ©duire modÃ¨le (gpt2)
4. Upgrade instance
```

---

## ğŸ’¡ RÃ©sumÃ© Final

**Render a limites RAM** â†’ Besoin stratÃ©gie alternative

**4 Options Valides:**
1. âœ… **ModÃ¨le lÃ©ger** (gpt2) - Gratuit, rapide, qualitÃ© OK
2. âœ… **Dynamic download** - Gratuit, 1er appel lent, rapide aprÃ¨s
3. âœ… **S3 storage** - $0.1/mois, rapide, flexible
4. âœ… **API externe** - ZÃ©ro serveur, pay-as-you-go

**Recommandation**: StratÃ©gie 2 (Dynamic) ou 1 (gpt2)
- ZÃ©ro coÃ»t supplÃ©mentaire
- Marche sur instance gratuite Render
- Acceptable pour diagnostics frigo

**Next Step**: Tester localement avant Render! ğŸš€
