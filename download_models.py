#!/usr/bin/env python3
"""
T√©l√©charger et sauvegarder tous les mod√®les IA localement
Pr√©pare les mod√®les pour d√©ploiement et r√©entra√Ænement futur

Usage:
    python download_models.py                    # T√©l√©charger tous
    python download_models.py --model phi2       # T√©l√©charger un seul
    python download_models.py --model mistral
    python download_models.py --model neural
    python download_models.py --model gpt2
"""

import os
import sys
import argparse
import logging
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Configuration des mod√®les
MODELS = {
    'phi2': {
        'hf_id': 'microsoft/phi-2',
        'size': '5GB',
        'description': 'Phi-2: Petit, rapide, performant (2.7B params)'
    },
    'mistral': {
        'hf_id': 'mistralai/Mistral-7B-Instruct-v0.1',
        'size': '13GB',
        'description': 'Mistral-7B: √âquilibr√©, haute qualit√© (7B params)'
    },
    'neural': {
        'hf_id': 'Intel/neural-chat-7b-v3-1',
        'size': '13GB',
        'description': 'Neural-Chat: Optimis√© pour chat (7B params)'
    },
    'gpt2': {
        'hf_id': 'openai/gpt2',
        'size': '500MB',
        'description': 'GPT-2: Fallback ultra-l√©ger (125M params)'
    }
}

# Dossier de destination
MODELS_DIR = Path(__file__).parent / 'models'

def download_model(model_name):
    """T√©l√©charger et sauvegarder un mod√®le"""
    
    if model_name not in MODELS:
        logger.error(f"‚ùå Mod√®le inconnu: {model_name}")
        logger.info(f"Mod√®les disponibles: {list(MODELS.keys())}")
        return False
    
    model_config = MODELS[model_name]
    hf_id = model_config['hf_id']
    size = model_config['size']
    desc = model_config['description']
    
    logger.info(f"\n{'='*70}")
    logger.info(f"üì• T√©l√©chargement {model_name.upper()}")
    logger.info(f"{'='*70}")
    logger.info(f"üìã {desc}")
    logger.info(f"üìä Taille estim√©e: {size}")
    logger.info(f"üîó Source: {hf_id}")
    
    # Cr√©er le dossier de destination
    model_dir = MODELS_DIR / model_name
    model_dir.mkdir(parents=True, exist_ok=True)
    logger.info(f"üìÅ Destination: {model_dir}")
    
    try:
        # V√©rifier si mod√®le existe d√©j√†
        if (model_dir / 'config.json').exists():
            logger.info(f"‚úÖ Mod√®le {model_name} existe d√©j√†")
            return True
        
        # T√©l√©charger tokenizer
        logger.info(f"‚è≥ √âtape 1/3: T√©l√©chargement tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            hf_id,
            trust_remote_code=True
        )
        logger.info(f"‚úÖ Tokenizer t√©l√©charg√©")
        
        # T√©l√©charger mod√®le
        logger.info(f"‚è≥ √âtape 2/3: T√©l√©chargement mod√®le (peut prendre plusieurs minutes)...")
        model = AutoModelForCausalLM.from_pretrained(
            hf_id,
            trust_remote_code=True,
            torch_dtype='auto',
            device_map='auto'
        )
        logger.info(f"‚úÖ Mod√®le t√©l√©charg√©")
        
        # Sauvegarder localement
        logger.info(f"‚è≥ √âtape 3/3: Sauvegarde locale...")
        tokenizer.save_pretrained(str(model_dir))
        model.save_pretrained(str(model_dir))
        logger.info(f"‚úÖ Mod√®le sauvegard√© dans {model_dir}")
        
        # V√©rifier la sauvegarde
        files = list(model_dir.glob('*'))
        logger.info(f"‚úÖ Fichiers sauvegard√©s: {len(files)} fichier(s)")
        for f in files[:5]:
            logger.info(f"   - {f.name}")
        
        logger.info(f"üéâ {model_name.upper()} pr√™t!")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erreur t√©l√©chargement {model_name}: {e}")
        return False

def main():
    parser = argparse.ArgumentParser(
        description='T√©l√©charger les mod√®les IA localement'
    )
    parser.add_argument(
        '--model',
        help=f'Mod√®le sp√©cifique √† t√©l√©charger. Disponibles: {", ".join(MODELS.keys())}',
        default=None
    )
    parser.add_argument(
        '--all',
        action='store_true',
        help='T√©l√©charger tous les mod√®les'
    )
    
    args = parser.parse_args()
    
    logger.info(f"ü§ñ Gestionnaire T√©l√©chargement Mod√®les IA")
    logger.info(f"üìÅ Dossier: {MODELS_DIR}")
    logger.info(f"")
    
    # Afficher les mod√®les disponibles
    logger.info("üìã Mod√®les disponibles:")
    for name, config in MODELS.items():
        logger.info(f"   - {name:10} ({config['size']:6}): {config['description']}")
    logger.info("")
    
    # D√©terminer quels mod√®les t√©l√©charger
    if args.all:
        models_to_download = list(MODELS.keys())
    elif args.model:
        models_to_download = [args.model]
    else:
        # Par d√©faut: phi2 + gpt2 (production + fallback)
        models_to_download = ['phi2', 'gpt2']
        logger.info("üí° Mode par d√©faut: phi2 + gpt2")
        logger.info("   Utilisez --all pour t√©l√©charger tous les mod√®les")
        logger.info("   Utilisez --model <name> pour un mod√®le sp√©cifique")
    
    logger.info(f"\nüì• √Ä t√©l√©charger: {', '.join(models_to_download)}")
    
    # T√©l√©charger les mod√®les
    results = {}
    for model_name in models_to_download:
        results[model_name] = download_model(model_name)
    
    # R√©sum√©
    logger.info(f"\n{'='*70}")
    logger.info(f"üìä R√âSUM√â")
    logger.info(f"{'='*70}")
    
    success_count = sum(1 for v in results.values() if v)
    total_count = len(results)
    
    for model_name, success in results.items():
        status = "‚úÖ" if success else "‚ùå"
        logger.info(f"{status} {model_name:10} - {MODELS[model_name]['size']:6}")
    
    logger.info(f"\n‚úÖ Succ√®s: {success_count}/{total_count}")
    
    if success_count == total_count:
        logger.info(f"\nüéâ Tous les mod√®les sont pr√™ts!")
        logger.info(f"üì¶ Vous pouvez maintenant pousser vers GitHub avec Git LFS")
        logger.info(f"üöÄ Et d√©ployer sur Render")
        return 0
    else:
        logger.error(f"\n‚ùå Certains mod√®les ont √©chou√©")
        return 1

if __name__ == '__main__':
    sys.exit(main())
