# ğŸ§ª AMÃ‰LIORATIONS TESTS SUGGÃ‰RÃ‰ES

Le fichier `tests/test_simple.py` est bon mais on peut l'amÃ©liorer. Voici comment :

---

## âš ï¸ PROBLÃˆME ACTUEL

Le fichier test_simple.py teste les fonctions, MAIS il y a une fonction manquante :

```python
from utils.helpers import calculer_score_sante_global
```

Cette fonction n'existe pas dans `utils/helpers.py` !

**Solution :** Ajouter cette fonction Ã  `utils/helpers.py`

---

## ğŸ”§ SOLUTION: AJOUTER LA FONCTION MANQUANTE

Ajoutez ceci Ã  `utils/helpers.py` :

```python
def calculer_score_sante_global(donnees: Dict, seuils: Dict) -> float:
    """
    Calcule un score de santÃ© global du systÃ¨me frigorifique
    
    Score 0-100:
    - 100 = parfait
    - 80+ = systÃ¨me sain
    - 50-80 = attention
    - <50 = problÃ¨me
    
    Args:
        donnees: DonnÃ©es capteurs actuelles
        seuils: Dict de seuils de rÃ©fÃ©rence
        
    Returns:
        Score 0-100
    """
    if not donnees or not seuils:
        return 50.0
    
    scores = []
    
    for capteur, seuil in seuils.items():
        if capteur not in donnees:
            scores.append(50)  # Capteur manquant = moyen
            continue
        
        valeur = donnees.get(capteur)
        min_val = seuil.get('min', 0)
        max_val = seuil.get('max', 100)
        optimal = seuil.get('optimal', (min_val + max_val) / 2)
        
        # VÃ©rifier si dans limites
        if valeur < min_val or valeur > max_val:
            scores.append(20)  # Hors limites = mauvais
        # VÃ©rifier Ã©cart par rapport optimal
        elif abs(valeur - optimal) > (max_val - min_val) * 0.3:
            scores.append(60)  # Loin de optimal = moyen
        else:
            scores.append(90)  # Proche optimal = bon
    
    # Moyenne des scores
    score_global = sum(scores) / len(scores) if scores else 50
    return round(score_global, 1)
```

---

## âœ¨ AMÃ‰LIORATIONS Ã€ AJOUTER

### 1ï¸âƒ£ Ajout du package pytest.ini

CrÃ©er `pytest.ini` pour configuration Pytest :

```ini
[pytest]
# Configuration Pytest
minversion = 7.0
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = -v --strict-markers
markers =
    unit: Tests unitaires
    integration: Tests d'intÃ©gration
    slow: Tests lents
```

---

### 2ï¸âƒ£ AmÃ©liorer test_simple.py

Remplacer le contenu par version amÃ©liorÃ©e :

```python
"""
Tests pour le systÃ¨me de diagnostic frigorifique
"""

import pytest
import json
import sys
from pathlib import Path

# Imports des modules Ã  tester
from utils.validation import valider_donnees_capteurs
from utils.helpers import (
    generer_diagnostic_id,
    calculer_score_sante_global,
    calculer_moyenne,
    detecter_anomalie
)
from config import Config


# ============================================================
# FIXTURES (Setup donnÃ©es)
# ============================================================

@pytest.fixture
def donnees_capteurs_valides():
    """DonnÃ©es de test valides"""
    return {
        'TempÃ©rature': -18.0,
        'Pression_BP': 2.5,
        'Pression_HP': 12.0,
        'IntensitÃ©_Compresseur': 15.0,
        'IntensitÃ©_Ventilateur': 5.0,
        'HumiditÃ©_Evaporateur': 60.0,
        'Vibrations': 1.0
    }


@pytest.fixture
def donnees_capteurs_incompletes():
    """DonnÃ©es incomplÃ¨tes (manquent 5 capteurs)"""
    return {
        'TempÃ©rature': -18.0,
        'Pression_BP': 2.5
    }


@pytest.fixture
def donnees_capteurs_anomalie():
    """DonnÃ©es avec une anomalie"""
    return {
        'TempÃ©rature': 50.0,  # âš ï¸ ANOMALIE!
        'Pression_BP': 2.5,
        'Pression_HP': 12.0,
        'IntensitÃ©_Compresseur': 15.0,
        'IntensitÃ©_Ventilateur': 5.0,
        'HumiditÃ©_Evaporateur': 60.0,
        'Vibrations': 1.0
    }


# ============================================================
# TEST SUITE 1: VALIDATION
# ============================================================

@pytest.mark.unit
class TestValidation:
    """Tests du service de validation"""
    
    def test_validation_donnees_valides(self, donnees_capteurs_valides):
        """Test validation avec donnÃ©es valides"""
        validated = valider_donnees_capteurs(donnees_capteurs_valides)
        
        assert validated is not None
        assert isinstance(validated, dict)
        assert validated['TempÃ©rature'] == -18.0
        assert validated['Pression_BP'] == 2.5
        assert len(validated) == 7
    
    def test_validation_donnees_manquantes(self, donnees_capteurs_incompletes):
        """Test validation avec donnÃ©es manquantes"""
        with pytest.raises(ValueError, match="DonnÃ©es insuffisantes"):
            valider_donnees_capteurs(donnees_capteurs_incompletes)
    
    def test_validation_donnees_vides(self):
        """Test validation avec dict vide"""
        with pytest.raises(ValueError):
            valider_donnees_capteurs({})
    
    def test_validation_none_input(self):
        """Test validation avec None"""
        with pytest.raises((ValueError, AttributeError)):
            valider_donnees_capteurs(None)


# ============================================================
# TEST SUITE 2: HELPERS
# ============================================================

@pytest.mark.unit
class TestHelpers:
    """Tests des fonctions utilitaires"""
    
    def test_generer_diagnostic_id_unique(self):
        """Test que IDs gÃ©nÃ©rÃ©s sont uniques"""
        ids = [generer_diagnostic_id() for _ in range(100)]
        
        # VÃ©rifier unicitÃ©
        assert len(ids) == len(set(ids)), "IDs ne sont pas uniques!"
    
    def test_generer_diagnostic_id_format(self):
        """Test format ID diagnostic"""
        id_diag = generer_diagnostic_id()
        
        assert id_diag.startswith('DIAG_')
        assert len(id_diag) > 10
    
    def test_calculer_moyenne(self):
        """Test calcul de moyenne"""
        valeurs = [10, 20, 30, 40, 50]
        moyenne = calculer_moyenne(valeurs)
        
        assert moyenne == 30.0
    
    def test_calculer_moyenne_vide(self):
        """Test moyenne liste vide"""
        moyenne = calculer_moyenne([])
        
        assert moyenne == 0.0
    
    def test_detecter_anomalie_positif(self):
        """Test dÃ©tection anomalie (positif)"""
        valeur = 100  # TrÃ¨s loin de moyenne
        moyenne = 10
        ecart = 5
        
        assert detecter_anomalie(valeur, moyenne, ecart, seuil=2.0) is True
    
    def test_detecter_anomalie_negatif(self):
        """Test dÃ©tection anomalie (nÃ©gatif)"""
        valeur = 11  # PrÃ¨s de moyenne
        moyenne = 10
        ecart = 5
        
        assert detecter_anomalie(valeur, moyenne, ecart, seuil=2.0) is False


# ============================================================
# TEST SUITE 3: CALCULS SANTÃ‰
# ============================================================

@pytest.mark.unit
class TestSante:
    """Tests calcul score santÃ©"""
    
    def test_score_sante_normal(self, donnees_capteurs_valides):
        """Test score santÃ© systÃ¨me normal"""
        score = calculer_score_sante_global(donnees_capteurs_valides, Config.SEUILS)
        
        assert isinstance(score, float)
        assert 0 <= score <= 100
        assert score > 70, f"Score devrait Ãªtre > 70, got {score}"
    
    def test_score_sante_anomalie(self, donnees_capteurs_anomalie):
        """Test score santÃ© avec anomalie"""
        score = calculer_score_sante_global(donnees_capteurs_anomalie, Config.SEUILS)
        
        assert isinstance(score, float)
        assert 0 <= score <= 100
        assert score < 70, f"Score devrait Ãªtre < 70, got {score}"
    
    def test_score_sante_empty(self):
        """Test score santÃ© donnÃ©es vides"""
        score = calculer_score_sante_global({}, Config.SEUILS)
        
        # Pas donnÃ©es = score moyen
        assert 0 <= score <= 100


# ============================================================
# TEST SUITE 4: INTEGRATION
# ============================================================

@pytest.mark.integration
class TestIntegration:
    """Tests d'intÃ©gration"""
    
    def test_workflow_complet(self):
        """Test workflow complet"""
        # 1. GÃ©nÃ©rer donnÃ©es
        donnees = {
            'TempÃ©rature': -18.0,
            'Pression_BP': 2.5,
            'Pression_HP': 12.0,
            'IntensitÃ©_Compresseur': 15.0,
            'IntensitÃ©_Ventilateur': 5.0,
            'HumiditÃ©_Evaporateur': 60.0,
            'Vibrations': 1.0
        }
        
        # 2. Valider
        validated = valider_donnees_capteurs(donnees)
        assert validated is not None
        
        # 3. GÃ©nÃ©rer ID
        diag_id = generer_diagnostic_id()
        assert diag_id.startswith('DIAG_')
        
        # 4. Calculer score
        score = calculer_score_sante_global(validated, Config.SEUILS)
        assert score > 70
        
        print(f"âœ… Workflow OK - ID: {diag_id}, Score: {score}")


# ============================================================
# MAIN
# ============================================================

if __name__ == '__main__':
    # Lancer les tests
    pytest.main([__file__, '-v', '-m', 'unit'])
```

---

## ğŸš€ NOUVEAUX TESTS Ã€ AJOUTER

### CrÃ©er `tests/test_services.py`

```python
"""Tests pour les services"""

import pytest
from services.gemini_service import GeminiService
from services.apprentissage_service import ApprentissageService


@pytest.mark.unit
class TestGeminiService:
    """Tests service Gemini"""
    
    def test_gemini_init_sans_key(self):
        """Test init sans clÃ© API"""
        service = GeminiService(api_key="")
        
        assert service.model is None
    
    def test_generer_fallback_analyse(self):
        """Test analyse fallback"""
        service = GeminiService(api_key="")
        result = service._generer_fallback_analyse()
        
        assert result['succes'] is False
        assert 'analyse' in result


@pytest.mark.unit
class TestApprentissageService:
    """Tests service apprentissage"""
    
    def test_init_apprentissage(self):
        """Test initialisation service"""
        service = ApprentissageService()
        
        assert service.compteur is not None
        assert service.compteur['total'] >= 0


if __name__ == '__main__':
    pytest.main([__file__, '-v'])
```

### CrÃ©er `tests/test_simulateur.py`

```python
"""Tests pour le simulateur"""

import pytest
from simulateur import SimulateurCapteurs


@pytest.mark.unit
class TestSimulateur:
    """Tests simulateur capteurs"""
    
    def test_init_simulateur(self):
        """Test initialisation"""
        sim = SimulateurCapteurs(prob_panne=0.3, interval=10)
        
        assert sim.prob_panne == 0.3
        assert sim.interval == 10
    
    def test_generer_capteurs_normaux(self):
        """Test gÃ©nÃ©ration capteurs normaux"""
        sim = SimulateurCapteurs()
        capteurs = sim.generer_capteurs_normaux()
        
        assert len(capteurs) == 7  # 7 capteurs
        assert all(isinstance(v, float) for v in capteurs.values())
    
    def test_generer_diagnostic(self):
        """Test gÃ©nÃ©ration diagnostic"""
        sim = SimulateurCapteurs()
        diag = sim.generer_donnees_diagnostic()
        
        assert 'timestamp' in diag
        assert 'num_diagnostic' in diag
        assert 'TempÃ©rature' in diag


if __name__ == '__main__':
    pytest.main([__file__, '-v'])
```

---

## ğŸ“Š STRUCTURE RECOMMANDÃ‰E

```
tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ conftest.py              â† Fixtures partagÃ©es
â”œâ”€â”€ pytest.ini               â† Config pytest
â”œâ”€â”€ test_simple.py           â† Tests validation + helpers
â”œâ”€â”€ test_services.py         â† Tests services IA (NEW)
â”œâ”€â”€ test_simulateur.py       â† Tests simulateur (NEW)
â”œâ”€â”€ test_api.py              â† Tests endpoints (NEW)
â””â”€â”€ fixtures/
    â””â”€â”€ test_data.json       â† DonnÃ©es de test
```

---

## ğŸ¯ CHECKLIST AMÃ‰LIORATIONS

- [ ] Ajouter `calculer_score_sante_global()` Ã  `utils/helpers.py`
- [ ] CrÃ©er `pytest.ini`
- [ ] AmÃ©liorer `test_simple.py` avec fixtures
- [ ] CrÃ©er `test_services.py`
- [ ] CrÃ©er `test_simulateur.py`
- [ ] CrÃ©er `test_api.py`
- [ ] Atteindre 80% couverture
- [ ] Ajouter tests CI/CD

---

## ğŸ’» COMMANDS

```bash
# Lancer tous les tests
pytest tests/ -v

# Lancer tests unitaires seulement
pytest tests/ -v -m unit

# Lancer tests d'intÃ©gration
pytest tests/ -v -m integration

# Voir couverture de code
pytest tests/ --cov --cov-report=html

# Lancer un fichier de test spÃ©cifique
pytest tests/test_services.py -v

# Watch mode (relance auto)
pytest-watch tests/
```

---

**Les tests = QualitÃ© assurÃ©e! ğŸ¯**
