# üîÑ Changer de Mod√®le IA sur Render - Guide Complet

## ‚ö†Ô∏è Attention: Distinction Importante

### **La Confusion**
```
‚ùå FAUX: Juste changer IA_MODEL env var ‚Üí Mod√®le change instantan√©ment
‚úÖ VRAI: D√©pend du mod√®le et de la configuration Dockerfile
```

---

## üìä Comprendre le Fonctionnement

### **Scenario 1: Mod√®le dans Dockerfile (Pr√©-compil√©)**

```
Dockerfile.production inclut:
  RUN python download_models.py --model phi

R√©sultat:
  ‚úÖ Image finale: 6.5GB avec phi pr√©-inclus
  ‚úÖ D√©marrage: Imm√©diat (~2 sec)
  
Si tu changes IA_MODEL env var:
  gpt2 ‚Üí phi (ou autre)
  ‚ö†Ô∏è NE change rien
  ‚Üí Mod√®le reste celui du Dockerfile
```

### **Scenario 2: Mod√®le T√©l√©charg√© √† la Demande (Dynamic)**

```
Dockerfile.render-lite:
  Pas de t√©l√©chargement dans Dockerfile
  IA_MODEL sp√©cifie le mod√®le √† utiliser

R√©sultat:
  ‚úÖ Image l√©g√®re: 500MB
  ‚è≥ Premier appel: ~30-60 sec (t√©l√©charge mod√®le)
  
Si tu changes IA_MODEL env var:
  gpt2 ‚Üí gpt2-medium ou distilgpt2
  ‚úÖ Change au red√©marrage
  ‚Üí App t√©l√©charge le nouveau mod√®le (30-60 sec)
  
Si tu changes IA_MODEL:
  gpt2 ‚Üí phi2 (13GB!)
  ‚ö†Ô∏è Possible mais attente: 2-5 min de t√©l√©chargement
  ‚ö†Ô∏è RAM Render peut crash (512MB insuffisant)
```

---

## üéØ 3 Approches (Choisis UNE)

### **Approche 1: Simple - Rester sur gpt2** ‚≠ê RECOMMAND√â

**Configuration Actuelle = Approche 1**

```
Dockerfile.render-lite:
  ENV IA_MODEL=gpt2

Comportement:
  ‚úÖ Toujours utilise gpt2
  ‚úÖ Pas de t√©l√©chargement extra
  ‚úÖ Rapide et stable
  
Si tu veux changer plus tard:
  Render ‚Üí frigo-gpt ‚Üí Environment
  IA_MODEL = distilgpt2 (ou autre l√©ger)
  Red√©ployer
  
Mod√®les compatibles (taille similaire):
  ‚Ä¢ gpt2-medium (650MB)
  ‚Ä¢ distilgpt2 (350MB)
  ‚Ä¢ gpt2-large (1.5GB) ‚ö†Ô∏è Possible mais lent
```

**‚úÖ √Ä faire MAINTENANT (rien!)** - C'est d√©j√† configur√©

---

### **Approche 2: Flexible - Plusieurs Mod√®les L√©gers**

**Permettre switch entre mod√®les sans red√©ployer le Dockerfile**

#### **Fichier √† modifier: `gpt/Dockerfile.render-lite`**

```dockerfile
FROM python:3.11-slim

WORKDIR /app

RUN apt-get update && apt-get install -y \
    gcc curl git \
    && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

RUN mkdir -p /app/logs /app/models /app/cache

# ============================================================
# FLEXIBLE: Supporte plusieurs mod√®les via env var
# ============================================================
ENV FLASK_APP=app_ia.py
ENV FLASK_ENV=production
ENV PYTHONUNBUFFERED=1

# ‚Üê CLEF: Pas de IA_MODEL par d√©faut dans Dockerfile
# La config viendra de Render Environment variables

ENV HF_HOME=/app/models
ENV IA_USE_GPU=false
ENV MAIN_APP_URL=http://app:5000
ENV CHAT_API_URL=http://chat:5001

EXPOSE 5002

HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
    CMD curl -f http://localhost:5002/health || exit 1

CMD ["python", "app_ia.py"]
```

#### **Modifie aussi `gpt/ia_service.py`**

Ajoute fallback intelligent au d√©but du fichier:

```python
import os

# D√©terminer le mod√®le √† utiliser
def get_model_name():
    """
    Priorit√©s de s√©lection:
    1. IA_MODEL env var (si sp√©cifi√©)
    2. Auto-s√©lection selon ressources
    """
    model = os.environ.get('IA_MODEL')
    
    if model:
        return model
    
    # Auto-s√©lection bas√©e sur m√©moire/GPU
    try:
        import psutil
        import torch
        
        total_mem = psutil.virtual_memory().total / (1024**3)  # GB
        has_gpu = torch.cuda.is_available()
        
        if has_gpu:
            return 'mistral-7b'
        elif total_mem >= 8:
            return 'phi-2'
        elif total_mem >= 4:
            return 'gpt2-medium'
        else:
            return 'gpt2'  # Fallback s√ªr
    except:
        return 'gpt2'  # Fallback ultime

IA_MODEL = get_model_name()
```

#### **Configuration Render pour Approche 2**

```
Service: frigo-gpt

Environment Variables:

IA_MODEL = gpt2                    # √Ä cr√©er MAINTENANT
MAIN_APP_URL = https://frigo-app.onrender.com
CHAT_API_URL = https://frigo-chat.onrender.com
```

**Plus tard, pour changer le mod√®le:**

```
1. Render Dashboard ‚Üí frigo-gpt
2. Environment ‚Üí Modifier IA_MODEL
3. Ancienne valeur: gpt2
   Nouvelle valeur: gpt2-medium (ou distilgpt2)
4. Save
5. Manual Deploy

R√©sultat:
  ‚úÖ Image rebuild? NON
  ‚úÖ Juste env var change
  ‚úÖ Au red√©marrage: t√©l√©charge nouveau mod√®le
```

**Mod√®les test√©s et recommand√©s:**

| Mod√®le | Size | RAM | Download | Qualit√© | Speed |
|--------|------|-----|----------|---------|-------|
| gpt2 | 500MB | 2GB | 20sec | Bonne | Rapide |
| distilgpt2 | 350MB | 1GB | 15sec | OK | Tr√®s Rapide |
| gpt2-medium | 650MB | 3GB | 30sec | Tr√®s bonne | Rapide |
| phi-2 | 5GB | 8GB | 120sec | Excellente | Normal |
| mistral-7b | 13GB | 16GB | 300sec | Excellente | Lent |

---

### **Approche 3: Hardcore - Pr√©-charger Multiple Mod√®les**

**T√©l√©charger plusieurs mod√®les d'avance dans Dockerfile**

```dockerfile
FROM python:3.11-slim as model-downloader

WORKDIR /app

RUN apt-get update && apt-get install -y build-essential git && rm -rf /var/lib/apt/lists/*

COPY requirements.txt /tmp/requirements.txt
COPY download_models.py /tmp/download_models.py

RUN pip install --no-cache-dir -r /tmp/requirements.txt

RUN mkdir -p /app/models && \
    python /tmp/download_models.py --model gpt2 && \
    python /tmp/download_models.py --model gpt2-medium && \
    echo "‚úÖ Tous les mod√®les t√©l√©charg√©s"

# Stage 2: Final image
FROM python:3.11-slim

WORKDIR /app
RUN apt-get update && apt-get install -y gcc curl && rm -rf /var/lib/apt/lists/*

COPY --from=model-downloader /app/models /app/models

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .
RUN mkdir -p /app/logs /app/cache

ENV FLASK_APP=app_ia.py
ENV FLASK_ENV=production
ENV HF_HOME=/app/models
ENV IA_MODEL=gpt2

EXPOSE 5002

CMD ["python", "app_ia.py"]
```

**R√©sultat:**
- ‚úÖ Build: 10-15 min (t√©l√©charge 2 mod√®les)
- ‚úÖ Image: 2GB
- ‚úÖ Switch instant entre gpt2 et gpt2-medium
- ‚ö†Ô∏è Prend plus d'espace que Approche 1

---

## üìã Recommandation Par Cas

### **Tu veux lancer en production MAINTENANT?**
üëâ **Approche 1 (Simple)** - ‚úÖ Configur√© par d√©faut
```
Utilise gpt2
Aucune config suppl√©mentaire
Pr√™t √† d√©ployer
```

### **Tu veux flexibilit√© pour changer plus tard?**
üëâ **Approche 2 (Flexible)** - ‚≠ê RECOMMAND√â
```
Ajoute IA_MODEL env var sur Render
Permet switch entre mod√®les l√©gers
Build rapide, pas d'image grosse
```

### **Tu veux performance maximale, co√ªt flexible?**
üëâ **Approche 3 (Multiple)** - Pro
```
Pr√©-charge plusieurs mod√®les
Switch instant
Plus d'espace, plus de temps build
```

---

## üîß Configuration Maintenant (√âtape par √âtape)

### **Choix: Je Vais Avec Approche 1 (Simple)**

```
‚úÖ Rien √† faire - c'est d√©j√† configur√©!

Fichiers actuels:
  ‚úÖ Dockerfile.render-lite (gpt/)  ‚Üí ENV IA_MODEL=gpt2
  ‚úÖ app_ia.py ‚Üí Utilise IA_MODEL env var
  ‚úÖ ia_service.py ‚Üí Auto-select fallback

Pour d√©ployer:
  1. Git push
  2. Cr√©er frigo-gpt sur Render
  3. Root: gpt/
  4. Build: docker build -f Dockerfile.render-lite .
  5. Env: IA_MODEL = gpt2
```

### **Choix: Je Vais Avec Approche 2 (Flexible)**

√Ä faire MAINTENANT:

**√âtape 1: Modifier `gpt/Dockerfile.render-lite`**

Supprimer la ligne:
```dockerfile
ENV IA_MODEL=gpt2
```

(Laisser vide - viendra de Render)

**√âtape 2: Ajouter code intelligent √† `gpt/ia_service.py`**

Au top du fichier:
```python
import os

def get_model_name():
    model = os.environ.get('IA_MODEL', 'gpt2')  # gpt2 par d√©faut
    return model
```

**√âtape 3: Configuration Render**

Environment variables:
```
IA_MODEL = gpt2  (d√©faut)
```

Puis tu peux changer plus tard:
```
IA_MODEL = gpt2-medium
IA_MODEL = distilgpt2
```

---

## üöÄ R√©sum√©: Feuille de Route

### **MAINTENANT (avant d√©ploiement)**

- [ ] D√©cider: Approche 1 (simple) ou 2 (flexible)?
- [ ] Si Approche 2: Modifier `gpt/Dockerfile.render-lite`
- [ ] Si Approche 2: Ajouter code `ia_service.py`
- [ ] Git push
- [ ] Cr√©er services Render (frigo-app, frigo-chat, frigo-gpt)

### **SUR RENDER (configuration)**

- [ ] frigo-gpt ‚Üí Environment ‚Üí IA_MODEL = gpt2
- [ ] Autres env vars: MAIN_APP_URL, CHAT_API_URL
- [ ] Deploy

### **PLUS TARD (si changement needed)**

```
Aller Render Dashboard:
  frigo-gpt ‚Üí Settings ‚Üí Environment
  
Modifier:
  IA_MODEL = gpt2       ‚Üí IA_MODEL = gpt2-medium
  
Sauvegarder ‚Üí Manual Deploy

App red√©marre avec nouveau mod√®le ‚úÖ
```

---

## ‚ö†Ô∏è Important: Limites Render

**Si tu changes vers mod√®le LOURD:**

```
Exemple:
  gpt2 (500MB) ‚Üí phi-2 (5GB)
  
‚ùå PROBL√àME:
  Render instance RAM: 512MB
  phi-2 besoin: 8GB minimum
  
R√âSULTAT:
  ‚úÖ Mod√®le commence t√©l√©charger
  ‚ö†Ô∏è App crash OOM pendant t√©l√©chargement
  ‚ùå Service down
  
SOLUTIONS:
  1. Utiliser Dockerfile.production
     (pr√©-compile phi dans l'image)
  2. Ou: Upgrade instance Render (payant)
  3. Ou: Utiliser S3 storage
```

---

## üí° Exemple Pratique

### **Tu veux passer de gpt2 ‚Üí gpt2-medium**

**Sur ta machine (local):**
```powershell
# Test localement d'abord
$env:IA_MODEL = "gpt2-medium"
python gpt/app_ia.py
# V√©rifie que √ßa fonctionne
```

**Sur Render:**
```
1. Dashboard ‚Üí frigo-gpt
2. Environment ‚Üí Modifier IA_MODEL
   Avant: gpt2
   Apr√®s: gpt2-medium
3. Save
4. Manual Deploy
5. Attendre: 30-60 sec (t√©l√©charge mod√®le)
6. V√©rifier logs: "‚úÖ Model loaded"
```

---

## üìû Troubleshooting

| Probl√®me | Cause | Solution |
|----------|-------|----------|
| **Mod√®le ne change pas** | Env var pas prise en compte | Red√©ployer (Manual Deploy) |
| **App crash avec OOM** | Mod√®le trop lourd | Utiliser mod√®le plus l√©ger |
| **T√©l√©chargement tr√®s lent** | Mod√®le lourd, bande √©troite | Normal, attendre |
| **Service reste en crash loop** | RAM insuffisant | Upgrade Render ou r√©duire mod√®le |

---

## ‚úÖ Checklist Finale

- [ ] D√©cid√©: Approche 1, 2 ou 3?
- [ ] Modifi√© Dockerfile si n√©cessaire
- [ ] Code intelligent pour auto-select?
- [ ] Mod√®les compatibles identifi√©s
- [ ] Pr√™t √† configurer Render
- [ ] Documentation sauvegard√©e

**Prochaine √©tape**: D√©ployer sur Render! üöÄ
