#!/usr/bin/env python3
"""
Fine-tuning des mod√®les IA avec donn√©es domaine frigo
√Ä ex√©cuter sur le serveur Render apr√®s d√©ploiement

Usage (Local):
    python fine_tune.py --model phi2 --data data/frigo_training.csv --epochs 3

Usage (Serveur Render):
    # Via SSH ou webhook
    python fine_tune.py --model phi2 --data /app/data/frigo_training.csv

R√©sultat:
    Mod√®le fine-tun√© sauvegard√© dans models/{model_name}-finetuned/
"""

import os
import sys
import argparse
import logging
import json
from pathlib import Path
from datetime import datetime

import torch
import numpy as np
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    TextDataset,
    DataCollatorForLanguageModeling,
    EarlyStoppingCallback
)

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class FinetuneConfig:
    """Configuration du fine-tuning"""
    
    MODELS = {
        'phi2': 'microsoft/phi-2',
        'mistral': 'mistralai/Mistral-7B-Instruct-v0.1',
        'neural': 'Intel/neural-chat-7b-v3-1',
        'gpt2': 'openai/gpt2'
    }
    
    # Hyperparam√®tres
    LEARNING_RATE = 2e-5
    BATCH_SIZE = 4  # R√©duit pour serveur (√©conomiser VRAM)
    NUM_EPOCHS = 3
    MAX_SEQ_LENGTH = 512
    WARMUP_STEPS = 100
    WEIGHT_DECAY = 0.01
    
    # Paths
    MODELS_DIR = Path(__file__).parent / 'models'
    DATA_DIR = Path(__file__).parent / 'data'
    OUTPUT_DIR = Path(__file__).parent / 'models'

class DataProcessor:
    """Traiter les donn√©es d'entra√Ænement"""
    
    @staticmethod
    def load_csv_data(csv_path, text_column='text'):
        """Charger donn√©es depuis CSV"""
        try:
            import pandas as pd
            df = pd.read_csv(csv_path)
            
            if text_column not in df.columns:
                logger.error(f"‚ùå Colonne '{text_column}' non trouv√©e")
                logger.info(f"Colonnes disponibles: {df.columns.tolist()}")
                return None
            
            # Combiner les textes
            texts = df[text_column].astype(str).tolist()
            logger.info(f"‚úÖ {len(texts)} exemples charg√©s depuis {csv_path}")
            
            return texts
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lecture CSV: {e}")
            return None
    
    @staticmethod
    def load_jsonl_data(jsonl_path, text_field='text'):
        """Charger donn√©es depuis JSONL (1 JSON par ligne)"""
        try:
            texts = []
            with open(jsonl_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    try:
                        data = json.loads(line)
                        if text_field in data:
                            texts.append(str(data[text_field]))
                    except json.JSONDecodeError as e:
                        logger.warning(f"‚ö†Ô∏è Ligne {line_num} invalide: {e}")
            
            logger.info(f"‚úÖ {len(texts)} exemples charg√©s depuis {jsonl_path}")
            return texts
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lecture JSONL: {e}")
            return None
    
    @staticmethod
    def save_training_file(texts, output_path):
        """Sauvegarder les textes pour l'entra√Ænement"""
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                for text in texts:
                    f.write(text + '\n')
            logger.info(f"‚úÖ Fichier d'entra√Ænement cr√©√©: {output_path}")
            return True
        except Exception as e:
            logger.error(f"‚ùå Erreur sauvegarde: {e}")
            return False

class ModelFinetuner:
    """Fine-tuner les mod√®les IA"""
    
    def __init__(self, model_name):
        self.config = FinetuneConfig()
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        logger.info(f"ü§ñ Fine-tuner: {model_name}")
        logger.info(f"üñ•Ô∏è Device: {self.device.upper()}")
        
        if self.device == "cuda":
            logger.info(f"üìä GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
    
    def load_model(self):
        """Charger le mod√®le"""
        try:
            hf_id = self.config.MODELS.get(self.model_name)
            if not hf_id:
                logger.error(f"‚ùå Mod√®le inconnu: {self.model_name}")
                return False
            
            # Essayer charger depuis local d'abord
            local_path = self.config.MODELS_DIR / self.model_name
            if local_path.exists():
                logger.info(f"üìÅ Mod√®le local trouv√©: {local_path}")
                model_id = str(local_path)
                use_local = True
            else:
                logger.info(f"üåê Mod√®le depuis HuggingFace: {hf_id}")
                model_id = hf_id
                use_local = False
            
            logger.info(f"‚è≥ Chargement tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_id,
                trust_remote_code=True,
                local_files_only=use_local
            )
            
            logger.info(f"‚è≥ Chargement mod√®le...")
            load_kwargs = {
                'trust_remote_code': True,
                'device_map': 'auto' if torch.cuda.is_available() else None,
            }
            
            if self.device == "cuda":
                load_kwargs['torch_dtype'] = torch.float16
            
            if use_local:
                load_kwargs['local_files_only'] = True
            
            self.model = AutoModelForCausalLM.from_pretrained(model_id, **load_kwargs)
            
            logger.info(f"‚úÖ Mod√®le charg√©: {self.model_name}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erreur chargement mod√®le: {e}")
            return False
    
    def prepare_dataset(self, train_file):
        """Pr√©parer le dataset pour l'entra√Ænement"""
        try:
            logger.info(f"üìä Pr√©paration dataset: {train_file}")
            
            # Load dataset
            dataset = TextDataset(
                tokenizer=self.tokenizer,
                file_path=train_file,
                block_size=self.config.MAX_SEQ_LENGTH
            )
            
            data_collator = DataCollatorForLanguageModeling(
                tokenizer=self.tokenizer,
                mlm=False
            )
            
            logger.info(f"‚úÖ Dataset pr√™t: {len(dataset)} exemples")
            return dataset, data_collator
            
        except Exception as e:
            logger.error(f"‚ùå Erreur pr√©paration dataset: {e}")
            return None, None
    
    def fine_tune(self, train_file, num_epochs=3, batch_size=4):
        """Lancer le fine-tuning"""
        try:
            # Pr√©parer le dataset
            dataset, data_collator = self.prepare_dataset(train_file)
            if dataset is None:
                return False
            
            # Output directory
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_dir = self.config.OUTPUT_DIR / f"{self.model_name}-finetuned-{timestamp}"
            output_dir.mkdir(parents=True, exist_ok=True)
            
            logger.info(f"üìÅ Output directory: {output_dir}")
            
            # Training arguments
            training_args = TrainingArguments(
                output_dir=str(output_dir),
                overwrite_output_dir=False,
                num_train_epochs=num_epochs,
                per_device_train_batch_size=batch_size,
                save_steps=100,
                save_total_limit=2,
                logging_steps=10,
                learning_rate=self.config.LEARNING_RATE,
                weight_decay=self.config.WEIGHT_DECAY,
                warmup_steps=self.config.WARMUP_STEPS,
                gradient_accumulation_steps=2,
                fp16=self.device == "cuda",  # Mixed precision
                dataloader_pin_memory=True,
                dataloader_num_workers=0,  # Important pour serveur
            )
            
            logger.info(f"‚è≥ D√©marrage du fine-tuning...")
            logger.info(f"   Epochs: {num_epochs}")
            logger.info(f"   Batch size: {batch_size}")
            logger.info(f"   Learning rate: {self.config.LEARNING_RATE}")
            
            # Trainer
            trainer = Trainer(
                model=self.model,
                args=training_args,
                data_collator=data_collator,
                train_dataset=dataset,
                callbacks=[EarlyStoppingCallback(
                    early_stopping_patience=3,
                    early_stopping_threshold=0.001
                )]
            )
            
            # Train
            result = trainer.train()
            
            logger.info(f"‚úÖ Fine-tuning compl√©t√©!")
            logger.info(f"üìä Loss final: {result.training_loss:.4f}")
            
            # Sauvegarder
            logger.info(f"üíæ Sauvegarde du mod√®le...")
            self.model.save_pretrained(str(output_dir))
            self.tokenizer.save_pretrained(str(output_dir))
            
            # Cr√©er un lien symbolique vers la version "latest"
            latest_dir = self.config.OUTPUT_DIR / f"{self.model_name}-finetuned"
            if latest_dir.exists():
                import shutil
                shutil.rmtree(latest_dir)
            latest_dir.symlink_to(output_dir)
            
            logger.info(f"üéâ Mod√®le fine-tun√© sauvegard√©!")
            logger.info(f"üìÅ Chemin: {output_dir}")
            logger.info(f"üîó Lien: {latest_dir} -> {output_dir}")
            
            # Sauvegarder les m√©tadonn√©es
            metadata = {
                'model_name': self.model_name,
                'timestamp': timestamp,
                'num_epochs': num_epochs,
                'batch_size': batch_size,
                'final_loss': float(result.training_loss),
                'device': self.device,
                'data_file': str(train_file)
            }
            
            with open(output_dir / 'finetuning_metadata.json', 'w') as f:
                json.dump(metadata, f, indent=2)
            
            logger.info(f"‚úÖ M√©tadonn√©es sauvegard√©es")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Erreur fine-tuning: {e}")
            import traceback
            traceback.print_exc()
            return False

def main():
    parser = argparse.ArgumentParser(
        description='Fine-tuner les mod√®les IA avec donn√©es domaine'
    )
    parser.add_argument(
        '--model',
        required=True,
        choices=['phi2', 'mistral', 'neural', 'gpt2'],
        help='Mod√®le √† fine-tuner'
    )
    parser.add_argument(
        '--data',
        required=True,
        help='Fichier d\'entra√Ænement (CSV ou JSONL)'
    )
    parser.add_argument(
        '--epochs',
        type=int,
        default=3,
        help='Nombre d\'epochs (default: 3)'
    )
    parser.add_argument(
        '--batch-size',
        type=int,
        default=4,
        help='Batch size (default: 4, r√©duire si OOM)'
    )
    parser.add_argument(
        '--text-column',
        default='text',
        help='Nom de la colonne texte (pour CSV)'
    )
    
    args = parser.parse_args()
    
    logger.info(f"{'='*70}")
    logger.info(f"ü§ñ Fine-tuning IA - Domaine Frigorifique")
    logger.info(f"{'='*70}")
    
    # V√©rifier le fichier de donn√©es
    data_path = Path(args.data)
    if not data_path.exists():
        logger.error(f"‚ùå Fichier non trouv√©: {args.data}")
        return 1
    
    logger.info(f"üìä Donn√©es: {data_path}")
    logger.info(f"ü§ñ Mod√®le: {args.model}")
    logger.info(f"‚è±Ô∏è  Epochs: {args.epochs}")
    logger.info(f"üì¶ Batch size: {args.batch_size}")
    
    # Charger les donn√©es
    logger.info(f"\n‚è≥ Chargement des donn√©es...")
    
    if data_path.suffix == '.csv':
        texts = DataProcessor.load_csv_data(str(data_path), args.text_column)
    elif data_path.suffix == '.jsonl':
        texts = DataProcessor.load_jsonl_data(str(data_path))
    else:
        logger.error(f"‚ùå Format non support√©: {data_path.suffix}")
        logger.info("Formats support√©s: .csv, .jsonl")
        return 1
    
    if not texts:
        logger.error(f"‚ùå Aucune donn√©e trouv√©e")
        return 1
    
    # Cr√©er fichier d'entra√Ænement
    train_file = data_path.parent / f"train_{args.model}.txt"
    if not DataProcessor.save_training_file(texts, train_file):
        return 1
    
    # Fine-tuner
    finetuner = ModelFinetuner(args.model)
    
    if not finetuner.load_model():
        return 1
    
    if not finetuner.fine_tune(train_file, args.epochs, args.batch_size):
        return 1
    
    logger.info(f"\n{'='*70}")
    logger.info(f"üéâ SUCC√àS!")
    logger.info(f"{'='*70}")
    logger.info(f"Mod√®le fine-tun√©: models/{args.model}-finetuned/")
    logger.info(f"Pr√™t pour production!")
    
    return 0

if __name__ == '__main__':
    sys.exit(main())
