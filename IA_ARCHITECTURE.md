# ü§ñ Architecture IA Modulaire - Guide Complet

## Vue d'ensemble

Le syst√®me IA supporte **plusieurs mod√®les** avec **s√©lection automatique** selon les ressources disponibles.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      Demande Message/Alerte             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ Auto-S√©lection   ‚îÇ
         ‚îÇ Ressources       ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                    ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Ollama‚îÇ  CPU/GPU ‚îÇHuggingFace
    ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                    ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇMod√®les Locaux‚îÇ    ‚îÇMistral/Phi2/IA ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ                    ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ  R√©ponse       ‚îÇ
         ‚îÇ  (Rapide/Bon)  ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìã Mod√®les Disponibles

### Production-Ready

| Mod√®le | Taille | Params | VRAM | Vitesse | Qualit√© | Use Case |
|--------|--------|--------|------|---------|---------|----------|
| **Phi-2** | 5GB | 2.7B | 2-4GB | ‚ö° Rapide | ‚≠ê‚≠ê‚≠ê‚≠ê | CPU/GPU Prod |
| **Mistral-7B** | 13GB | 7B | 8-16GB | ‚≠ê Bon | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | GPU Haute M√©m |
| **Neural-Chat** | 13GB | 7B | 8-16GB | ‚≠ê Bon | ‚≠ê‚≠ê‚≠ê‚≠ê | Chat Optimis√© |
| **Ollama** | Variable | - | Faible | ‚ö°‚ö° Tr√®sRapide | ‚≠ê‚≠ê‚≠ê‚≠ê | Local Engine |

### Fallback

| Mod√®le | Usage |
|--------|-------|
| **GPT-2** | Dev/test uniquement (g√©n√©raliste) |
| **R√©ponses Intelligentes** | Mode d√©grad√© (template-based) |

## üöÄ S√©lection Automatique

Le syst√®me d√©tecte automatiquement les ressources:

```python
# 1. Ollama disponible?
if ollama_running:
    model = 'ollama'

# 2. GPU disponible?
elif gpu_available:
    if gpu_memory > 10GB:
        model = 'mistral'  # GPU haute m√©moire
    else:
        model = 'phi2'     # GPU limit√©e
        
# 3. CPU seulement
else:
    model = 'phi2'        # Optimis√© CPU

# 4. Fallback
if model_fails:
    model = 'r√©ponses_intelligentes'
```

## üéØ Configuration par Environnement

### D√©veloppement (CPU local)
```bash
# Auto-s√©lection (d√©tecte CPU, utilise phi2)
python app_ia.py

# Ou forcer gpt2 (test rapide)
export IA_MODEL=gpt2
python app_ia.py
```

### Production (GPU disponible)
```bash
# Auto-s√©lection (d√©tecte GPU, utilise mistral/phi2)
python app_ia.py

# Ou forcer mistral
export IA_MODEL=mistral
python app_ia.py

# Ou forcer ollama
export IA_MODEL=ollama
python app_ia.py
```

### Docker/Render
```dockerfile
# Dans Dockerfile, sp√©cifier le mod√®le production
ENV IA_MODEL=phi2           # Pour CPU
# ou
ENV IA_MODEL=mistral        # Pour GPU
# ou
ENV IA_MODEL=ollama         # Si ollama disponible
```

## üîÑ Mod√®les Locaux

Placer les mod√®les dans `models/{folder}`:

```
models/
‚îú‚îÄ‚îÄ phi-2/              # Phi-2 2.7B (5GB)
‚îú‚îÄ‚îÄ mistral-7b/         # Mistral-7B (13GB)
‚îú‚îÄ‚îÄ neural-chat-7b/     # Neural-Chat (13GB)
‚îú‚îÄ‚îÄ gpt2/               # GPT-2 125M (500MB)
‚îî‚îÄ‚îÄ ollama/             # (Utilise API ollama)
```

**T√©l√©charger les mod√®les:**

```bash
# Phi-2 (recommand√© pour prod CPU)
python -c "from transformers import AutoTokenizer, AutoModelForCausalLM; \
    model_dir = Path('models/phi-2'); \
    model_dir.mkdir(parents=True, exist_ok=True); \
    tokenizer = AutoTokenizer.from_pretrained('microsoft/phi-2'); \
    model = AutoModelForCausalLM.from_pretrained('microsoft/phi-2'); \
    tokenizer.save_pretrained(str(model_dir)); \
    model.save_pretrained(str(model_dir))"

# Mistral-7B (pour GPU haute m√©moire)
python -c "from transformers import AutoTokenizer, AutoModelForCausalLM; \
    model_dir = Path('models/mistral-7b'); \
    model_dir.mkdir(parents=True, exist_ok=True); \
    tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.1'); \
    model = AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.1'); \
    tokenizer.save_pretrained(str(model_dir)); \
    model.save_pretrained(str(model_dir))"
```

## üîÆ R√©entra√Ænement Futur

L'architecture est pr√™te pour le fine-tuning avec donn√©es domaine frigo:

```python
# Pseudocode pour r√©entra√Ænement futur
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir='./models/phi-2-finetuned-frigo',
    num_train_epochs=3,
    per_device_train_batch_size=4,
)

trainer = Trainer(
    model=base_model,  # phi2, mistral, etc.
    args=training_args,
    train_dataset=frigo_dataset,  # Donn√©es domaine frigo
    callbacks=[EarlyStoppingCallback()]
)

trainer.train()
```

**Donn√©es requises pour r√©entra√Ænement:**
- ‚úÖ Historique diagnostics
- ‚úÖ Q&A domaine frigo
- ‚úÖ Manuels techniques
- ‚úÖ Alertes et solutions

## üìä Logs et Monitoring

Lors du d√©marrage, le service affiche:

```
üîç D√©tection des ressources disponibles...
‚úÖ GPU disponible: 8.0GB
üìä GPU m√©moire limit√©e -> Phi-2
üìã Configuration: Phi-2: Petit, rapide, bon (2.7B params)
üñ•Ô∏è Device: CUDA
‚è≥ Chargement tokenizer...
‚è≥ Chargement mod√®le phi2...
‚úÖ Mod√®le phi2 charg√© avec succ√®s
```

## üîó APIs de R√©ponse

Tous les mod√®les partagent la **m√™me interface d'API**:

```json
POST /api/chat/message
{
    "message": "Qu'est-ce qu'une panne √©lectrique?",
    "user_id": "123",
    "user_name": "admin"
}

Response:
{
    "success": true,
    "response": "‚ö° Probl√®me d'alimentation d√©tect√©...",
    "intent": "diagnostic",
    "model": "phi2",
    "device": "cuda",
    "processing_time_ms": 2340
}
```

## üõ†Ô∏è Troubleshooting

### Ollama n'est pas d√©tect√©
```bash
# V√©rifier si ollama est lanc√©
curl http://localhost:11434/api/tags

# Lancer ollama
ollama serve
```

### GPU non d√©tect√©
```bash
# V√©rifier CUDA
python -c "import torch; print(torch.cuda.is_available())"

# Installer CUDA drivers
# https://developer.nvidia.com/cuda-downloads
```

### Mod√®le trop lent
- CPU: Utiliser phi2 (4-5s) au lieu de mistral (15-20s)
- GPU: V√©rifier VRAM, peut √™tre limit√©

### Mod√®le ne charge pas
- V√©rifier chemin local: `ls models/phi-2/`
- V√©rifier connectivit√© HF: `curl https://huggingface.co`
- V√©rifier espace disque

## üìà √âvolution Pr√©vue

1. ‚úÖ Multi-mod√®les support
2. ‚úÖ Auto-s√©lection ressources
3. ‚è≥ Fine-tuning donn√©es frigo
4. ‚è≥ Caching r√©ponses
5. ‚è≥ A/B testing mod√®les
6. ‚è≥ Monitoring performances
7. ‚è≥ Fallback API distante (Claude/GPT-4)
