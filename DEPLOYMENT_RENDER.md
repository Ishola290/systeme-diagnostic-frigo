# Guide de D√©ploiement sur Render ‚Äî 3 Services Microservices

## Architecture D√©ploy√©e

```
Service 1: app (Main) - Port 5000
‚îú‚îÄ‚îÄ app.py
‚îú‚îÄ‚îÄ services/
‚îî‚îÄ‚îÄ Dockerfile (racine)
    
Service 2: chat (Web UI) - Port 5001
‚îú‚îÄ‚îÄ chat/app_web.py
‚îú‚îÄ‚îÄ chat/Dockerfile
‚îî‚îÄ‚îÄ Connect√© √†: app + gpt

Service 3: gpt (IA Service) - Port 5002
‚îú‚îÄ‚îÄ gpt/app_ia.py
‚îú‚îÄ‚îÄ gpt/Dockerfile
‚îî‚îÄ‚îÄ Mod√®le: models/phi-2/ (local, pr√©-t√©l√©charg√©)
```

Les trois services communiquent via **URLs internes Render** (noms DNS du service).

---

## Pr√©-requis

‚úÖ D√©p√¥t GitHub avec Git LFS configur√© (mod√®les phi-2 inclus)  
‚úÖ Compte Render (https://render.com)  
‚úÖ Trois services configur√©s (suivre les √©tapes ci-dessous)

---

## √âtapes de D√©ploiement

### √âtape 1 : Cr√©er Service 1 (Main App - app.py)

1. Connecte-toi √† **render.com**
2. Clique **New +** ‚Üí **Web Service**
3. S√©lectionne ton d√©p√¥t GitHub `systeme-diagnostic-frigo`
4. Configure comme suit :

   - **Name** : `systeme-diagnostic-main`
   - **Environment** : `Docker`
   - **Dockerfile Path** : `Dockerfile` (racine)
   - **Port** : `5000`
   - **Environment Variables** :
     ```
     FLASK_ENV=production
     PYTHONUNBUFFERED=1
     HF_LOCAL_MODEL_PATH=/app/models/phi-2
     CHAT_API_URL=http://systeme-diagnostic-chat:5001
     IA_SERVICE_URL=http://systeme-diagnostic-gpt:5002
     TELEGRAM_BOT_TOKEN=<ton_token_telegram>
     TELEGRAM_CHAT_ID=<ton_chat_id>
     ```

5. Clique **Create Web Service**

**Attends que le d√©ploiement se termine** (~5-10 min).  
L'URL sera quelque chose comme : `https://systeme-diagnostic-main.onrender.com`

---

### √âtape 2 : Cr√©er Service 2 (Chat Web - chat/app_web.py)

1. Clique **New +** ‚Üí **Web Service**
2. S√©lectionne le m√™me d√©p√¥t
3. Configure comme suit :

   - **Name** : `systeme-diagnostic-chat`
   - **Environment** : `Docker`
   - **Dockerfile Path** : `chat/Dockerfile`
   - **Port** : `5001`
   - **Environment Variables** :
     ```
     FLASK_ENV=production
     PYTHONUNBUFFERED=1
     MAIN_APP_URL=http://systeme-diagnostic-main:5000
     IA_SERVICE_URL=http://systeme-diagnostic-gpt:5002
     DATABASE_URL=sqlite:////tmp/chat_app.db
     SECRET_KEY=<une_cl√©_secr√®te_longue_al√©atoire>
     ```

4. Clique **Create Web Service**

**Attends la fin du d√©ploiement.**  
L'URL sera : `https://systeme-diagnostic-chat.onrender.com`

---

### √âtape 3 : Cr√©er Service 3 (IA Service - gpt/app_ia.py)

1. Clique **New +** ‚Üí **Web Service**
2. S√©lectionne le m√™me d√©p√¥t
3. Configure comme suit :

   - **Name** : `systeme-diagnostic-gpt`
   - **Environment** : `Docker`
   - **Dockerfile Path** : `gpt/Dockerfile`
   - **Port** : `5002`
   - **Environment Variables** :
     ```
     FLASK_ENV=production
     PYTHONUNBUFFERED=1
     HF_LOCAL_MODEL_PATH=/app/../models/phi-2
     IA_MODEL=phi
     IA_USE_GPU=false
     MAIN_APP_URL=http://systeme-diagnostic-main:5000
     CHAT_API_URL=http://systeme-diagnostic-chat:5001
     ```

4. Clique **Create Web Service**

**Attends la fin du d√©ploiement.**  
L'URL sera : `https://systeme-diagnostic-gpt.onrender.com`

---

## Communications Entre Services

| Service | Acc√®de √† | Via URL |
|---------|----------|---------|
| **main** | chat | `http://systeme-diagnostic-chat:5001` |
| **main** | gpt | `http://systeme-diagnostic-gpt:5002` |
| **chat** | main | `http://systeme-diagnostic-main:5000` |
| **chat** | gpt | `http://systeme-diagnostic-gpt:5002` |
| **gpt** | main | `http://systeme-diagnostic-main:5000` |
| **gpt** | chat | `http://systeme-diagnostic-chat:5001` |

Les noms DNS (ex: `systeme-diagnostic-main`) sont des alias internes Render ‚Äî pas besoin d'IP publiques.

---

## V√©rification du D√©ploiement

### 1. V√©rifier les Logs

Pour chaque service, clique sur le service ‚Üí **Logs** (en bas √† droite) :

‚úÖ **App (5000)** :
```
‚úÖ Service IA initialis√©
üöÄ D√©marrage Flask app
```

‚úÖ **Chat (5001)** :
```
‚úÖ Base de donn√©es initialis√©e
üöÄ D√©marrage du serveur web Flask
```

‚úÖ **GPT (5002)** :
```
üìÅ Chargement mod√®le depuis chemin par d√©faut: .../models/phi-2
‚úÖ Mod√®le phi charg√© avec succ√®s sur CPU
‚úÖ Service IA initialis√©
```

### 2. Tester les Health Checks

Depuis un terminal ou navigateur :

```bash
# Service Main
curl https://systeme-diagnostic-main.onrender.com/health

# Service Chat
curl https://systeme-diagnostic-chat.onrender.com/

# Service GPT
curl https://systeme-diagnostic-gpt.onrender.com/health
```

### 3. Tester la Communication Inter-Services

Envoie un message via le Chat Web :
- La requ√™te traverse : Chat (5001) ‚Üí GPT (5002) ‚Üí Main (5000)
- Si tout fonctionne, tu re√ßois une r√©ponse du mod√®le IA

---

## Probl√®mes Courants

### ‚ùå Le service GPT ne trouve pas le mod√®le

**Cause** : Les fichiers `models/phi-2/*.safetensors` n'ont pas √©t√© push√©s avec Git LFS.

**Solution** :
```bash
git lfs install
git lfs track "models/**/*.safetensors"
git add models/
git commit -m "Add models with LFS"
git push
```

Red√©ploie sur Render.

### ‚ùå Les services ne communiquent pas

**Cause** : Mauvaises URLs d'environnement.

**Solution** : V√©rifie que les noms des services dans les URLs correspondent exactement aux **Name** configur√©s sur Render. Par ex:
- Si le service chat s'appelle `my-chat-app`, l'URL doit √™tre `http://my-chat-app:5001`

### ‚ùå Chat Web ne d√©marre pas (erreur DB)

**Cause** : `init_db.py` √©choue.

**Solution** : 
1. V√©rifie que `chat/init_db.py` existe et cr√©e la DB correctement.
2. En Render, les fichiers persistent dans `/tmp/` ou montages volumes (√† configurer si besoin).

### ‚ùå GPU non disponible (normal sur Render Free)

**Cause** : Render Free n'a pas de GPU.

**Solution** : Le service GPT roule en mode CPU (c'est configur√© par `IA_USE_GPU=false`).  
Performance r√©duite mais acceptable pour Phi-2 (2.7B param√®tres).

---

## Mise √† Jour du Code

Quand tu pusses du nouveau code vers `main` :

1. **Render d√©tecte le push** automatiquement.
2. **Chaque service red√©ploie ind√©pendamment** (bas√© sur le Dockerfile qui a chang√©).
3. **Les services restent accessibles** pendant la construction (~ 5-15 min).

Pour forcer un red√©ploiement sans changement :
- Render ‚Üí Service ‚Üí **Deployment** ‚Üí **Manual Deploy** ‚Üí **Deploy latest commit**

---

## Scale / Upgrade

Si tu veux optimiser les performances :

- **Chat (5001)** : Monter les ressources (plus de RAM pour les sessions utilisateur).
- **GPT (5002)** : Upgrade √† Render's **Paid Plan** pour GPU (Tesla K80 ou mieux) ‚Üí Performance √ó5-10 pour l'IA.
- **Main (5000)** : Scale les workers si beaucoup d'utilisateurs.

---

## R√©sum√© des URLs Publiques

| Service | URL |
|---------|-----|
| **Main App** | `https://systeme-diagnostic-main.onrender.com` |
| **Chat Web UI** | `https://systeme-diagnostic-chat.onrender.com` |
| **IA Service API** | `https://systeme-diagnostic-gpt.onrender.com` |

Les services se trouvent **mutuellement** via URLs internes (ex: `http://systeme-diagnostic-gpt:5002`).

---

## Questions / Support

Si tu as des soucis :
1. V√©rifie les **Logs** pour chaque service.
2. Teste les **URLs internes** depuis le terminal du service.
3. Confirme que Git LFS a bien pouss√© les mod√®les (lfs pointer files vs binary).

Bon d√©ploiement ! üöÄ
